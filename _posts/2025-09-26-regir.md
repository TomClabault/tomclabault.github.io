---
layout: post
title: ReGIR - An advanced implementation for many-lights offline rendering
date: 2025-09-26 14:45:00+0200
description: 
tags: path-tracing
thumbnail: assets/img/blogs/regir/thumbnail.jpg
categories: hiprt-path-tracer
related_posts: false
related_publications: true
#pretty_table: true
toc:
  beginning: true
---





<!-- Scripts for the ImageBox -->
<link rel="stylesheet" href="/assets/css/distill-width-override.css">
<link rel="stylesheet" href="/assets/css/ImageBox/ImageBox.css">
<link rel="stylesheet" href="/assets/css/pygments-github.css">
<script src="/assets/js/ImageBox/ImageBox.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataRISNoVisibility.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataRegirLightSamplesPerReservoir.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataRegirGridSizes.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataHashGridCollisions.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataNewTargetFunctionQuality.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataRegirHashGridWithNormals.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/regirHashGridWithNormals.js"></script>
<!-- Scripts for the ImageBox -->






# The many-lights sampling problem

# Resampled Importance Sampling (RIS)



# ReGIR : Reservoir-based Grid Importance Resampling

## The algorithm

## A better grid

## Improving the target function
	
## NEE++

## BSDF resampling

## Reducing grid artifacts
		
## Spatial reuse

## Temporal reuse

## Fixing the bias

## Multiple Importance Sampling

## Reducing correlations

## Improving the base samples
	
## VRAM optimizations for cache cells

## Improvements to visibility noise

# ReSTIR x ReGIR

# Performance and memory usage

# Limitations and potential improvements

## Correlations
		
## Choosing the grid size







# The many-lights sampling problem


To render a 3D scene, a naive "brute-force" path tracer sends out rays from the camera, bounce the rays around the scene multiple
times and hopes fingers crossed that the ray is eventually going to hit a light. In the event that the ray leaves the scene without having
hit a light, the pixel stays black and you've done the work for nothing. On top of that if the lights of your scene are very small or very far
away (or more generally, are small in solid angle at your shading point), then naively bouncing rays around is going to have a very low
probability to actually hit a light source.

The solution to that in practice, rather than hoping that the rays are going to hit a light, is to purposefully choose a light from all the lights
that exist in your scene, shoot a shadow ray towards that light to see if it is occluded, and if it's not, use that light to shade the current vertex
along your path. This method is called next-event estimation (NEE).

However, it isn't that simple. The subtle question is: how do I choose the light that I shoot a shadow ray to? Do we just choose the light completely at random?
That works, but from an efficiency standpoint, this is far from ideal, especially when there are tons of lights in the scene: 
- I want to choose a light for my point $$X$$ on a surface
- There are 1000 lights in the scene
- Only 1 light meaningfully illuminates my point $$X$$, all the other 999 lights are too far away and don't contribute much at all to $$X$$

If I choose the lights for NEE completely at random, there's a 1/1000 chance that I'm going to choose the good light for the point $$X$$. That's not amazing and
this is going to cause very high variance (i.e. very high levels of noise) in our image and our render is going to take a while to converge to a clean image.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/twr-uniform-1SPP-vs-ref.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: one sample per pixel, choosing lights completely at random for NEE.<br>
	Bottom: reference, converged image.<br>
	This scene contains ~137k emissive triangles.
</div>

Surely this is going to take a while to converge.

> ##### Note
>
> Unless stated otherwise, all images in this blog post are rendered with 0 bounces (shoot a camera ray, find the first hit, do next-event estimation and you're done). 
> This is to focus on the variance of our NEE estimator. Adding more bounces would introduce the variance of path sampling and it would be harder to
> evaluate whether we're going in the right direction or not with regards to reducing the variance of our NEE estimator.
{: .block-tip }

Brief detour to the mathematics of why we have so much noise when choosing lights uniformly at random.

Ultimately, when estimating direct lighting using next-event estimation, we're trying to solve the following integral at each point $$P$$ in our scene:

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{}\rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}dA_x
\label{directLightingIntegral}
\end{equation}

With:
- $$L_o(P, \omega_o)$$ the reflected radiance of point $$P$$ in direction $$\omega_o$$, the **outgoing light** direction (direction towards the camera for example)
- $$\int_{A}$$ is the integral over the surface of our lights in the scene: to estimate the radiance that our point $$P$$ receives from the lights of the scene, we need a way of taking into account
all the lights of the scene. One way of doing that is to take all the lights and then consider all the points on the surface of these lights.
"Considering all the points of the surfaces of all the lights" is what we're doing here when integrating over $$A$$. $$A$$ can then be thought of as the union of
the area of all the lights and we're taking a point $$dA_x$$ on this union of areas.
- $$\rho(P, \omega_o, \omega_i)$$ is the BSDF used to evaluate the reflected radiance at point $$P$$ in direction $$\omega_o$$ of the **ingoing light** direction $$\omega_i$$ (which is $$x - P$$).
- $$L_e(x)$$ is the emission intensity of the point $$x$$
- $$V(P\leftrightarrow x)$$ is the visibility term that evaluates whether our point $$P$$ can see the point $$x$$ on the surface of a light or not (is it occluded by some geometry in the scene).
- $$cos(\theta)$$ is the attenuation term
- $$cos(\theta_L)/d^2$$ is the geometry term with $$d$$ the distance to the point on the light
- $$x$$ is a point on the surface of a light source

For brevity, we'll use

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{} f(x)dx
\label{directLightingIntegralBrevity}
\end{equation}

with

\begin{equation}
f(x) = \rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{directLightingIntegralBrevityFx}
\end{equation}

In a path tracer, we usually compute the value of this integral with monte carlo integration:
- Pick a sample (a point on a light) $$x$$ with some probability distribution function (PDF) $$p(x)$$
- Evaluate $$\frac{f(x)}{p(x)}$$

The average value of many of those samples will get closer and closer to the true value of equation **(\ref{directLightingIntegral})** which is the value that we want to compute.

Ideally, our PDF $$p(x)$$ is proportional to f(x): $$p(x) = c * f(x)$$. If this is the case, then every value $$\frac{f(x)}{p(x)}$$ that we compute is going to be the constant $$c$$:

\begin{equation}
\frac{f(x)}{p(x)} = \frac{f(x)}{c * f(x)} = c
\label{ZeroVarianceEq}
\end{equation}

In other words, no matter what random sample $$x$$ we evaluate, we will always get the same value $$c$$ out. This means that our estimator is going to have 0 variance.
And with a zero-variance-estimator we get no noise in the image (the noise that we see is just a consequence of neighboring pixels having quite a different value because of variance, even 
though nearby pixels on the image should be probably be almost the exact same color) and the whole image converges with a single sample per pixel. Very yummy.

So the ultimate goal is to be able to sample points on lights with a distribution $$p(x)$$ that follows $$f(x)$$ **(Eq. \ref{directLightingIntegralBrevityFx})**. This means, in order of the terms of $$f(x)$$:

1. $$\rho(P, \omega_o, \omega_i)$$: we want our sample $$x$$ to follow the shape of the BSDF: don't sample a point on a light that results in a $$\omega_i$$ that is outside of the delta peak of a specular BSDF for example
2. $$L_e(x)$$: sample lights proportionally to their power, the more powerful the more chance the light should have to be sampled
3. $$V(P\leftrightarrow x)$$: sample only visible points; don't sample a point on a light that is occluded from the point of view of $$P$$
4. $$cos(\theta)$$: sample light directions while accounting for the cosine-weighted falloff at the shading point $$P$$; directions closer to the surface normal contribute more than grazing ones
5. $$\frac{cos(\theta_L)}{d^2}$$: favor light samples that are both closer to $$P$$ (stronger due to distance attenuation) and oriented toward $$P$$ (the lightâ€™s surface normal aligned with direction $$\omega_i$$)

Which of those terms do we take into account when choosing a light completely at random for NEE? None of those. Hence why it's so bad.

A much better technique already is to sample lights according to their power: before rendering starts, compute the power of all the lights of the scene and build a CDF, an alias table (or anything that allows sampling
a CDF) on that list of light power. Lights can then be sampled exactly proportional to their power, meaning that we've gained the second term, $$L_e(x)$$.

The results are already much better than uniform sampling:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/twr-uniform-1SPP-vs-power-vs-ref.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: same as before: uniform sampling.<br>
	Center: power proportional sampling<br>
	Bottom: reference, converged image<br>
	This scene contains ~137k emissive triangles.
</div>

This is still far from perfect obviously. 3 areas area of particular interest:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/zoom-above-lamp-indexed.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Inset on the lamp
</div>

The first one is above (and below) the lamp as shown in the screenshot above. Looking at the reference image, the lamp inside the lampshade should clearly illuminate
the wall behind it, above and below the lampshade but we have almost nothing here. The reason for that is the same as for the chimney below: lights are not sampled based on
their distance ($$1/d^2$$ in term 5. $$\frac{cos(\theta_L)}{d^2}$$) to the shading point. Relative to the whole scene, the lights inside the lampshade and inside the chimney are not really powerful. The consequence of that is
that our power-proportional sampling does not favor those lights very much (because other lights are more powerful) even though those are the lights contributing the most to the
shading points above and below the lampshade and inside the chimney respectively. Those are the lights that our sampling strategy should have sampled here.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/zoom-chimney-indexed.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Inset on the chimney. The lack of spatial-aware sampling (and visibility sampling) is the cause for the chimney looking almost all black at 1SPP.
</div>

For the case of the chimey, the lack of information about visibility (term 3.$$V(P\leftrightarrow x)$$) is also an issue. Power sampling alone would not be too bad of an idea
if the large light panels in the front and the back of the room were visible from inside the chimney, but that's not the case. Those large light panels
are sampled often even though they are occluded from the inside of the chimney: our sampling distribution is far from proportional to our actual integrand $$f(x)$$
and we suffer from lots of noise because of that (we only have term 2. $$L_e(x)$$ after all).

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/back-of-couch.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: inset on the back of the couch <br>
	Bottom: Another point of view of the scene. The large quad light in the back is the only light that is able to light to back of the couch.
</div>

The case of the back of the couch is also interesting. Looking at the different point of view of the scene, we can see that the only light that is illuminating the back of the couch
is the large light panel in the back of the room. All the other lights of the scene are behind the back of the couch (they are in the back of the back of the couch.... ** visible confusion **).
This is a case where sampling according to term 4. $$cos(\theta)$$ would help immensely because that term here would be negative for all lights except the large light panel that matters to us.

The BSDF term 1. $$\rho(P, \omega_o, \omega_i)$$ will be tackled later. For the ones screaming "light hierarchies" in the back, we'll have a look at those later too.

# Resampled Importance Sampling (RIS)

Let's now introduce Resampled Importance Sampling (RIS).

RIS{% cite RISTalbot %} <d-cite key="RISTalbot"></d-cite> is at the heart of ReGIR. At its core, it is a technique that takes multiple samples $$(X_1, ..., X_M)$$ produced according to a **source distribution $$p$$** and "resamples" them
into one single sample $$Y$$ that follows a distribution $$\overline{p}$$ proportional to a **given (user-defined) target function $$\hat{p}$$**. Said otherwise, RIS takes a bunch of samples $$(X_1, ..., X_M)$$ (which can
be produced by our simple uniform sampling or power sampling routine from before for example) and outputs one new sample $$Y$$ that is distributed closer to whatever function $$\hat{p}$$ we want (to be precise the sample $$Y$$
will follow $$\overline{p}$$ closer, not $$\hat{p}$$ which may not be normalized and thus may not be a PDF).

Let's quickly dive into a concrete example applied to our many-light sampling problem before this gets all too theoretical:

Recall that the integral that we want to solve at each point $$P$$ in our scene is:

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{}\rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}dA_x
\label{directLightingIntegralRIS}
\end{equation}

Now again, what a naive path tracer does to solve this integral is: at each point $$P$$ in the scene, choose a light $$l$$ randomly (uniformly, proportional to power, light hierarchies for the ones in the back, ...),
choose a point $$x$$ on the surface of this light and evaluate equation **(\ref{directLightingIntegralRIS})**.

With the lights chosen uniformly, the probability of chosing the light $$l$$ is $$\frac{1}{\| Lights\|}$$ and the probability of uniformly choosing the point $$x$$ on the surface of that light $$l$$ is $$\frac{1}{Area(l)}$$
with $$\|Lights\|$$ the number of lights in the scene. 

Our **source distribution $$p$$** is then defined as:

\begin{equation}
p(x) = \frac{1}{\|Lights\| * Area(l)}
\label{sourceDistributionP}
\end{equation}

That explains the first part of the sentence about RIS: "it is a technique that takes multiple samples $$(X_1, ..., X_M)$$ produced according to a **source distribution $$p$$**". We've got our source distribution $$p$$
and we can generate samples (points on light sources) with it. 

The next step is then to use RIS to "resample them into one sample that follows a distribution $$\overline{p}$$ proportional to a **given (user-defined) target function $$\hat{p}$$**."

So first of all, what's going to be our user-defined target function $$\hat{p}$$? Why not choose equation **(\ref{directLightingIntegralRIS})** to be our target function so that RIS outputs
samples that are proportional exactly to the function that we're trying to integrate (which is ultimately the goal)? We can totally do that:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-4k-samples.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	1SPP with RIS, resampling 4000 power-proportional light samples (X_1, ..., X_4000) with the full f(x) as the target function pHat
</div>

Delectable.

This runs at 0.5FPS. 

Not so delectable anymore. 

In effect, doing this will not be efficient, mostly because of the visibility term that requires tracing a ray, which is an expensive operation. In practice, the target function $$\hat{p}$$ needs to be cheap-ish to evaluate or
RIS will turn out to be computationally inefficient. This worked okay for this scene because it's not too expensive to trace. And because power-proportional sampling does not do a catastrophically bad job
at sampling the scene to begin with. The same 4000-samples-RIS NEE strategy but using uniform sampling this time isn't as shiny:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-4k-uniform-samples.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	1SPP with RIS, resampling 4000 uniform ight samples (X_1, ..., X_4000). Bad. <br>
	This show the importance of the "base sampling strategy" that the samples we feed RIS are produced with. If our **source distribution p** is very bad, RIS will have a hard time getting a good sample out of it.
</div>

So if the full equation **(\ref{directLightingIntegralRIS})** is too expensive because of the visibility term, let's define our target function $$\hat{p}$$ without that visibility term:

\begin{equation}
\hat{p}(x) = \rho(P, \omega_o, \omega_i)L_e(x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{targetFunctionPHatNoVis}
\end{equation}

This
is what such a target function can give us:

<div id="ImageBoxContent" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("ImageBoxContent");
	
	if (dataRISNoVisibility['imageBoxes'])
		new ImageBox(content, dataRISNoVisibility['imageBoxes'], 1279, 692);


	// if (dataRISNoVisibility['stats'])
		//new ChartBox(content, dataRISNoVisibility['stats']);
		//new TableBox(content, dataRISNoVisibility['stats']);
</script>

There are a couple of interesting things happening here with that $$\hat{p}$$ from equation **(\ref{targetFunctionPHatNoVis})**:
1. The 40000 samples RIS render doesn't look that much better than the 1000 one
2. The 40000 samples RIS render looks worse in some places than the 10 samples RIS render (mostly around the lampshade)

For 1., the likely explanation is that at M=1000 samples, the sample $$Y$$ output by RIS is already distributed pretty much perfectly 
(almost perfectly. Looking closer at the back of the couch you see that this is still a bit noisy. The noise here comes from term 5. $$\frac{cos(\theta_L)}{d^2}$$ of equation **(\ref{directLightingIntegralBrevityFx})**)
according to $$\hat{p}(x) = \rho(P, \omega_o, \omega_i)L_e(x)cos(\theta)\frac{cos(\theta_L)}{d^2}$$.
Adding more samples won't make a difference since the sample is already perfect. It's a perfect sample but only with regards to that $$\hat{p}$$ without the visibility term $$V$$. 
Because we omitted that visibility term, the distribution according to which our samples $$Y$$ are produced is still not completely proportional to our ultimate goal, equation **(\ref{directLightingIntegralBrevityFx})**:

\begin{equation}
f(x) = \rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{directLightingIntegralBrevityFxAgain}
\end{equation}

That lack of proportionality to the visibility is the cause for all the rest of the noise visible in the image. Who would have guessed that visibility was important...

For 2., looking closer at the area around the green point on the left of the image, it's noticeable that RIS with 10 candidates (M = 10) looks a bit better than M = 40000:

<div class="col-sm mt-3 mt-md-0">
	{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-no-vis-inset-10-10000-green-spot.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: M = 10 <br>
	Bottom: M = 40000
</div>

This is because at M = 40000, our sample $$Y$$ is distributed much closer to $$\overline{p}$$ 
(remember that $$\hat{p}(x)$$ isn't necessarily a PDF, it's not necessarily normalized. So strictly speaking, the samples $$Y$$ are distributed closer to the normalized version of $$\hat{p}(x)$$, which is $$\overline{p}$$)
than with M = 10. However, this does not play in our favor here because of the missing visibility term. In this exact instance, getting closer to $$\hat{p}$$ without the visibility term $$V$$ (equation **(\ref{targetFunctionPHatNoVis})**) gets us
further away from our goal, equation **(\ref{directLightingIntegralBrevityFxAgain})**:

<div class="col-sm mt-3 mt-md-0">
	{% include figure.liquid path="assets/img/blogs/regir-many-lights/RIS-curves.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	RIS produces samples Y that are closer and closer to the target distribution pHatBar (pHatBar, our target function pHat without visibility, but normalized) as M
	increases but this also moves us further away from the perfect distribution pf(x) (f(x) but normalized such that it is a PDF) <br>
	
	<br>
	The X axis represents all possible light samples in the scene. <br>
	The Y axis is the probability that a given distribution produces that light sample <br>
	<br>
	
	These curves are approximate, they are just to illustrate the idea.
</div>

The curves above show us that increasing M gets us closer to $$\overline{\hat{p}}$$ but in some places, this also moves us further away from the true goal which is distributing our samples $$Y$$ proportional to $$f(x)$$, as we can see
with the dip in $$pf(x)$$ on the right of the graph. That dip is caused by the light in the lampshade being occluded from the point of view of the green point. RIS will keep choosing that light because it's close to our
green point but it is occluded so most samples will end up having 0 contribution in the end and this is the cause for the variance here.

Now to get back to our numerical example of how to actually use RIS, what do we do with our target function $$\hat{p}$$? We're going to use it to "resample" our samples $$(X_1, ..., X_M)$$ produced from $$p$$.

Here's the pseudocode of the RIS algorithm:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/RISAlgo.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Resampled Importance Sampling (RIS) pseudo-code algorithm. Source: "A Gentle Introduction to ReSTIR", 2023<d-cite key="Wyman2023Gentle"></d-cite>
</div>

Algorithm starts at line 10. The idea is simple.

- (Line 10-11) Generate our $$M$$ candidates according to the distribution $$p$$ we talked about earlier. This corresponds to picking points on the surface of the lights of our scene.
- (Line 12) Compute the "resampling weight" $$w_i$$ of this candidate $$X_i$$. 
	- $$m_i$$ is a MIS weight. For now, we'll use $$m_i=\frac{1}{M}$$
	- $$\hat{p}(X_i)$$ is the value of the target function for the generated sample $$X_i$$
	- $$W_{X_i}$$ is what's called the "unbiased contribution weight" of the sample/candidate. This term will be thoroughly detailed in the next sections.
	For now, we'll use $$W_{X_i}=\frac{1}{p(X_i)}$$, the inverse of the probability of picking *that* point $$X_i$$ on *that* light in the scene the point belongs to.
- (Line 14) With these weights $$w_i$$, we are now going to choose a sample $$X_i$$ proportionally to its weight $$w_i$$: 
	- Let's say we have $$M=3$$ candidates. We compute their resampling weights $$w_i$$ and obtain $$(w_1, w_2, w_3)=(1.2, 2.5, 0.5)$$. The *randomIndex()* function can then be
	used to obtain the sample $$X_i$$.
	Concretely in this example, the probability of choosing $$X_1$$, $$X_2$$ or $$X_3$$ with the random index $$s$$ is then:
	
$$ p(s=0)=\frac{1.2}{1.2+2.5+0.5}=0.2857 $$

$$ p(s=1)=\frac{2.5}{1.2+2.5+0.5}=0.595 $$

$$ p(s=2)=\frac{2.5}{1.2+2.5+0.5}=0.119 $$

With the random index $$s$$ chosen, we can retrieve the final sample $$Y$$: the output of RIS. This is with this sample $$Y$$ that we can evaluate NEE.

Now, in Monte Carlo integration, we need the probability (PDF) of sampling a given sample. This also applies to next event estimation as you may know: when you
sample a light in the scene, you divide by the probability of sampling that light when evaluating the radiance contribution of that light at the point you're shading.

RIS is no exception to that. RIS returns one light sample $$Y$$ and its weight $$W_Y$$ from the $$M$$ light samples that we fed it. If we want to use that light
sample $$Y$$ for evaluating NEE, we're going to need its PDF. The hard truth is that RIS "mixes $$M$$ samples together" and as a result,
we can't<d-cite key="bitterli2020spatiotemporal"></d-cite> easily compute the PDF of the resulting sample $$Y$$. What we can compute however, is an estimate of
its PDF. That's what the $$W_Y$$ computed at line 17 of the algorithm is. This estimate converges to the inverse PDF $$\frac{1}{\overline{p}(Y)}$$ of $$Y$$ as the
number of candidates $$M$$ resampled increases. If we then want to divide by the PDF of our sample $$Y$$, we can instead multiply by that estimate of the inverse
PDF $$W_Y$$ and achieve the same result.

As a matter of fact, a sample output by RIS can be used for evaluation as simply as:

\begin{equation}
f(Y)W_Y
\end{equation}

that is: evaluate the function you want to integrate with the sample $$Y$$ and multiply by $$W_Y$$ (the estimate of its inverse PDF $$\frac{1}{\overline{p}(Y)}$$) instead of dividing by $$p(x)$$
as we would do with non-RIS sampling schemes.

For our direct lighting evaluation case, we can use $$Y$$ as:

\begin{equation}
L_o(P, \omega_o) = f(P, \omega_o, \omega_i)V(P\leftrightarrow Y)cos(\theta)\frac{cos(\theta_L)}{d^2}W_Y
\end{equation}

with $$\omega_i$$ the direction towards the light sample (point on the surface of a light) $$Y$$ from $$P$$.

So to recap:
- RIS takes $$M$$ samples as input
- Those M samples are distributed according to a given distribution $$p$$ that is easy to sample from (power sampling in the screenshots above)
- Evaluate the weights $$w_i$$ for all these samples
- Choose one sample $$Y$$ from $$(X_1, ..., X_M)$$ proportional to its weight
- Compute $$W_Y$$ for that sample $$Y$$
- Evaluate NEE with that sample $$Y$$

And this is done at every shading point in the scene.

# ReGIR : Reservoir-based Grid Importance Resampling

## The algorithm

ReGIR is a light sampling technique built on top of RIS that was first described in the paper by Boksansky et al., 2021<d-cite key="boksansky2021regir"></d-cite>. 
BoissÃ©, 2021<d-cite key="boisse2021worldspaceRestir"></d-cite> also proposed something similar.
The idea is to have a grid (let's assume regular grid for now) built over your scene, and then, in a prepass to the rendering process:
1. For each cell of the grid, consider the point at the center of the grid
2. Sample a few lights, $$L = 32$$ ($$L$$ for lights) for example, using a simple sampling technique (power sampling for example)
3. Resample those $$L$$ lights with RIS by estimating the contribution of each of those $$L$$ samples to the point at the center of the grid cell
4. This outputs one light sample that is stored in the grid cell
5. Repeat the process a certain number of times, $$R = 64$$ ($$R$$ for reservoirs) to get more light samples (reservoirs) in each grid cell
6. At path tracing time, when evaluating NEE, find out which grid cell your shading point falls into
7. Get a few lights from that grid cell, $$S = 4$$ ($$S$$ for shading) for example.
8. Resample those $$S$$ light samples again with RIS
9. This outputs one final light sample
10. Shade that light sample for NEE

Essentially what this accomplishes is that it precomputes a bunch of light samples per each grid cell ahead of time, and then we can use these precomputed light samples
at shading time for NEE, instead of our regular sampling technique.

The good bit here is the target function that we can use in step 2. If we call $$P$$ the center of our cell, we can use:

\begin{equation}
\hat{p}(x) = \frac{L_e(x)}{|P - x|^2}
\label{regirGridFillTargetFunction}
\end{equation}

that is, a target function that not only contains the power of the light but also the distance to the light. The result is that our grid cells are going to
contain $$R$$ light samples (or reservoirs as we're using reservoirs to store the output of RIS here, as proposed in the ReGIR paper) that take power and distance into account. 
These power-distance light samples are then used at path tracing time for NEE (by looking up in which grid cell our shading point is) and completely replace the simpler power-proportional samples that we've been using
until now. So now our light sampling technique can take distance into account to produce its samples. Let's see how much of a quality difference that makes:

TTTTTTOOOOOOOOOOOOODDDDDDDOOOOOOOOOOOOO we want a light tree comparison in there

<div id="regir-light-samples-per-reservoir" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-light-samples-per-reservoir");
	
	if (dataRegirLightSamplesPerReservoir['imageBoxes'])
		new ImageBox(content, dataRegirLightSamplesPerReservoir['imageBoxes'], 1279, 692);
</script>

Quick note: For now, NEE is performed with a single light sample per shading point $$S = 1$$, as opposed to using RIS as in the introduction to RIS or using $$S = 4$$ as in step 7. 

Commenting on the white room results first, the behavior is a bit similar to what we had with RIS and the 40000 samples, except it's worse. The lampshade at the ceiling is almost completely black past $$L = 16$$!
And that's not buggy, the image will converge correctly eventually, that's just very very bad variance. That's because the light samples that we use for NEE are now 
distributed according to the simple target function of equation **(\ref{regirGridFillTargetFunction})**, which misses almost all the terms of our ultimate
goal, equation **(\ref{directLightingIntegralBrevityFx})**. Compared to simple power sampling, we gained the distance term $$\frac{1}{d^2}$$ but not the other terms and we get into a similar situation
as before (in the introduction to RIS section where we used $$\hat{p}$$ without the visibility term) where, in some places of the scene, RIS actually moves us further away from the ideal distribution than power sampling is.
That's the case for the exterior of the lampshade where most samples end up sampling the light inside the lampshade but that light is actually behind the surface normal and occluded: all bad samples. But
getting access to spatial sampling did help in the chimney area and above and below the blue lampshade on the right of the image.

On the Bistro, things look quite a bit better for ReGIR and increasing $$L$$ results in better and better samples for NEE. That Bistro scene suffers less from that issue of "RIS moving us away from the
ideal target distribution" and ReGIR now significantly outperforms power sampling at equal sample count (just 1SPP in the above screenshots). We'll have equal-time comparisons later.

Yet, those renders are still far from perfect, especially in the white room. So can we do better?

## A better grid

Let's start by addressing the elephant that has been hiding behind the couch: the regular grid.
	
Bad. Mostly because there is a ton of empty space in a regular grid: grid cells that contain no scene geometry at all and that our NEE is never going to query because no ray will ever
hit geometry in those grid cells. This has us filling the grid (computational time) and storing reservoirs in those grid cells (VRAM usage) for nothing. And a lot of them.
	
The better data structure that I ended up using for ReGIR is a hash grid, inspired by the spatial hasing of Binder et al. 2019<d-cite key="binderPSF2019"></d-cite>.
This works by first hashing a "descriptor", getting a hash key out of that hash operation, and using that hash key to index a buffer in memory: that index in the buffer is where we can store
information about the grid cell: the reservoirs of our cells and more.

Probably the most simple and useful descriptor that can be used to start with is just the 3D position in the scene:

```c++
unsigned int custom_regir_hash(float3 world_position, float cell_size, unsigned int total_number_of_cells, unsigned int& out_checksum) const
{
	// The division by the cell_size gives us control over the precision of the grid. Without that, each grid cell would be size 1^3
	// in world-space
	unsigned int grid_coord_x = static_cast<int>(floorf(world_position.x / cell_size));
	unsigned int grid_coord_y = static_cast<int>(floorf(world_position.y / cell_size));
	unsigned int grid_coord_z = static_cast<int>(floorf(world_position.z / cell_size));

	// Using the two hash functions as proposed in [WORLD-SPACE SPATIOTEMPORAL RESERVOIR REUSE FOR RAY-TRACED GLOBAL ILLUMINATION, Boisse, 2021]
	unsigned int checksum = h2_xxhash32(grid_coord_z + h2_xxhash32(grid_coord_y + h2_xxhash32(grid_coord_x)));
	unsigned int cell_hash = h1_pcg(grid_coord_z + h1_pcg(grid_coord_y + h1_pcg(grid_coord_x))) % total_number_of_cells;

	out_checksum = checksum;
	return cell_hash;
}
```

The hash function computes both a hash-key 'cell_hash' that will be used to index the has grid buffer and a checksum that can be used to detect collisions: it can happen that two very different
positions in world space end up yielding the same 'cell_hash', but chances are that the 'checksum' is going to be different, allowing us to detect the collision. If it happens that the checksum
is also the same, that's very unfortunate on top of that the checksum and we'll end up using the reservoirs of one grid cell to shade another cell in the grid. This is not the end of the world but
that's going to increase variance when that happens. 'checksum' uses a full 32 bit unsigned integer however so this situation should happen extremely infrequently. 'cell_hash' on the other hand is
computed modulo 'total_number_of_cells'. This is such that we do not get an index out of the hash function that does not fit in our hash grid buffer ('total_number_of_cells'
is the size of our hash grid buffer, allocated to a fixed size in advance)).

Resolving collisions is done with linear probing as presented by Binder et al. in their talk <d-cite key="binderPSF2019talk"></d-cite>., it's very simple although could use a bit more effectiveness. My testings suggest
that the hash table starts to struggle a bit with collisions at ~85% load factor (load factor is the proportion of cells occupied in the table). Considering that my tests were done with a maximum
of 32 steps for linear probing, that's not amazing when compared to the results of the survey on GPU hash tables from Awad et al. 2021 <d-cite key="awad2021bettergpuhashtables"></d-cite>. Their introduction report
a probe count of only 1.43 at load factor 99% for a BCHT (bucketed cuckoo hash table) scheme, but linear probing is already struggling at load factor 95% with a probe count of 32...

The consequence of a probing scheme that struggles is that we need to maintain a relatively low load factor to avoid suffering from unresolved collisions and lower performance because of longer probe
sequences. This means more wasted VRAM as the hash grid buffer must be larger than necessary to achieve a low enough load factor. My implementation resizes the buffer of the hash grid (and reinserts
the grid cells of the old hash table in the new hash table) automatically if the load factor exceeds 60% and I've found this enough of a "target load factor" in practice.

<div id="regir-hash-grid-collisions" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-hash-grid-collisions");
	
	if (dataHashGridCollisions['imageBoxes'])
		new ImageBox(content, dataHashGridCollisions['imageBoxes'], 1279, 692);
</script>

The above screenshots show that the number of collisions left unresolved increases as the load factor increases. Unresolved collisions manifest as black cells in the debug view. When doing NEE, this means that
we will not be able to access the hash grid data at all for that cell. That cell will thus be completely black as NEE will not have any light sample to work with. The solution that I've found to
that is just to fallback to a default light sampling strategy if that happens. This will have higher variance than ReGIR obviously (that's the goal after all, we want ReGIR to be better than just a
simple light sampling strategy) but at least it will not be conspicuously biased.

One that question that I myself have at this point: what's the impact of the hash grid resolution on quality and rendering efficiency? Well, let's have a look. A visualization of the grid sizes used
for this comparison is given below:

<div id="regir-hash-grid-size-quality" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-hash-grid-size-quality");
	
	if (dataRegirGridSizes['imageBoxes'])
		new ImageBox(content, dataRegirGridSizes['imageBoxes'], 1279, 692);
</script>

And a table to sum up the number of samples needed for convergence to a given quality target (given below) as well as the time it took to render that many samples:


<style>
.table_component {
    overflow: auto;
    width: 100%;
}

.table_component table {
    border: 1px solid #dededf;
    height: 100%;
    width: 100%;
    table-layout: fixed;
    border-collapse: collapse;
    border-spacing: 1px;
    text-align: center;
}

.table_component caption {
    caption-side: top;
    text-align: center;
}

.table_component th {
    border: 1px solid #dededf;
    background-color: #eceff1;
    color: #000000;
    text-align: center;
    padding: 5px;
}

.table_component td {
    border: 1px solid #dededf;
    background-color: #ffffff;
    color: #000000;
    padding: 5px;
}
</style>
<div class="table_component" role="region" tabindex="0">
<table>
    <caption>Rendering efficiency function of the grid resolution. ReGIR v1</caption>
    <thead>
        <tr>
            <th><br></th>
            <th><br></th>
            <th><br></th>
            <th><br></th>
            <th>Time to converge</th>
            <th><br></th>
            <th><br></th>
            <th>Power sampling only</th>
            <th>TOOOOOOOOOOOOODDDDDDOOOOOOOO WE WANT LIGHT TREE HERE</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Grid size:</td>
            <td>8</td>
            <td>2</td>
            <td>1</td>
            <td>0.5</td>
            <td>0.25</td>
            <td>0.125</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td><b>The white room (75%)</b></td>
            <td>10.13s</td>
            <td>13.86s</td>
            <td>14.53s</td>
            <td>15.32s</td>
            <td>15.93s</td>
            <td>19.08s</td>
            <td>8.6s</td>
            <td></td>
        </tr>
        <tr>
            <td><b>Bistro (25%)</b></td>
            <td>6.7s</td>
            <td>5.44s</td>
            <td>5.41s</td>
            <td>5.77s</td>
            <td>7.12s</td>
            <td>11.37s</td>
            <td>48.23s</td>
            <td></td>
        </tr>
		<tr>
            <td></td>
            <td></td>
            <td></td>
            <td><br></td>
            <td><b></b></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td></td>
            <td></td>
            <td></td>
            <td><br></td>
            <td><b>Samples to converge</b></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td>Grid size:</td>
            <td>8</td>
            <td>2</td>
            <td>1</td>
            <td>0.5</td>
            <td>0.25</td>
            <td>0.125</td>
            <td></td>
            <td></td>
        </tr>
        <tr>
            <td><b>The white room (75%)</b></td>
            <td>3257</td>
            <td>4395</td>
            <td>4565</td>
            <td>4691</td>
            <td>4712</td>
            <td>4717</td>
            <td>3166</td>
            <td></td>
        </tr>
        <tr>
            <td><b>Bistro (25%)</b></td>
            <td>1519</td>
            <td>1214</td>
            <td>1180</td>
            <td>1171</td>
            <td>1164</td>
            <td>1162</td>
            <td>13041</td>
            <td></td>
        </tr>
    </tbody>
</table>
</div>

At this point of the implementation, the grid size affects quality for the worse in the white room, noticeably, most likely because of the very imperfect target function that we're using and the white room scene
seems very sensitive to it. On the Bistro however, we do so an increase in quality (lower sample count to converge) until we hit diminishing returns in between grid size 1 and grid size 0.5. 
The increase in quality is directly visible in some areas of the Bistro where spatiality matters (and thus a higher grid resolution matters):

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/bistro-grid-size-spatiality-double.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Left: grid size 0.5<br>
	Right: grid size 8<br>
	Both 32SPP. L=32, R=64, S=1.
</div>

Reminder: $$L$$ is the number of light samples per reservoir of each grid cell. $$R$$ is the number of reservoirs per each grid cell. $$S$$ is how many reservoirs we resample per shading point at NEE time.

In terms of methodology for measuring what's reported in the table above, the scenes are rendered and frames accumulated until a certain proportion of the pixels of the image
have reached a given estimated variance threshold. For both scenes above, the variance threshold used was 0.075. Rendering stopped after 75% have reached that threshold variance in the white room
and 25% for the Bistro (indicated in the brackets).

In the end, this hash grid saves us a ton of VRAM but also, very importantly, allows us to have geometric information about the cells of the grid: because we allocate cells in the buffer when a ray hits
the scene (we hash the position and insert in the right place if that grid cell hasn't been inserted in the hash grid before), we only ever store grid cells on the actual geometry of the scene
and we know what that geometry is like since we used that geometric information to compute the hash in the first place. That geometric information attached to the grid cells is something that will come
in very handy for improving the quality of ReGIR samples.

## Improving the target function

So far, the target function that we've been using to resample our light samples $$L$$ into reservoirs $$R$$ during grid fill has been:

\begin{equation}
\hat{p}(x) = \frac{L_e(x)}{|P - x|^2}
\label{regirGridFillTargetFunctionAgain}
\end{equation}
	
We would like this target function to get as close as possible to the full $$f(x)$$ ideal target function. With the regular grid, we had to estimate
the contribution of the $$L$$ lights sampled to the center of the grid cell because we couldn't really do better. With the hash grid now, we have access to way more
information about the grid cell. For the ray that inserted that grid cell in the hash grid, we have:

- A point on the surface of the scene in that grid cell $$P_c$$ (for **P**oint_**c**ell)
- The surface normal at that point $$N_c$$ (for **N**ormal_**c**ell)
- The material at that point $$M_c$$ (for **M**aterial_**c**ell)

Note that if multiple rays hit the same hash grid cell and all want to insert their point/normal/material into the grid cell, only one of those rays will
win the atomic insertion operation and so the grid cell will be "geometrically represented" only by one point/normal/material = only one of all those rays that hit the cell.
This can cause some issues as we'll see just below.

With these new pieces of information, let's review which terms we'll be able to incorporate into our grid-fill target function:

- Term 1. $$\rho(P_c, \omega_o, \omega_i)$$ of $$f(x)$$ can be added since we have the material of the grid cell (the material at the point in the grid cell rather). We will still omit this one for now and reserve it for a later dedicated section of this blog post.
- Term 2. $$L_e(x)$$: we already had this one even with the regular grid
- Term 3. $$V(P_c\leftrightarrow x)$$: we can have the visibility term by tracing a ray between the point of the grid cell and the point of the light sample (we'll see that this is not a super good idea however)
- Term 4. $$cos(\theta)$$: we have the surface normal so this one fits too
- Term 5. $$\frac{cos(\theta_L)}{d^2}$$: we already had $$\frac{1}{d^2}$$ although this was with respect to the center of the cell. We can now compute $$\frac{1}{d^2} = \frac{1}{|P_c - x|^2}$$ 
given a point on the surface of the scene in the grid cell so that's going to be more precise. For $$cos(\theta_L)$$, we could already have added this one even with the 
regular grid, by computing it also with the respect to the center of the grid cell, but the original ReGIR article didn't use it so we started without it as well. We'll add
it to our new target function now.

With that, this is our new target function, for a light sample $$x$$ (point on a light):

\begin{equation}
\hat{p}(x) = L_e(x)V(P_c\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{|P_c - x|^2}
\label{regirGridFillTargetFunctionBetter}
\end{equation}

with 

- $$cos(\theta) = dot(N_c, normalized(x - P_c))$$.
- $$cos(\theta_L) = dot(normalized(P_c - x, N_L))$$, where $$N_L$$ is the normal at point $$x$$ on the surface of the light source

Where are we now in terms of quality with that new target function?

TTTTTTOOOOOOOOOOOOODDDDDDDOOOOOOOOOOOOO we want a light tree comparison in there

<div id="regir-nex-target-function-quality" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-nex-target-function-quality");
	
	if (dataNewTargetFunctionQuality['imageBoxes'])
		new ImageBox(content, dataNewTargetFunctionQuality['imageBoxes'], 1279, 692);
</script>

ReGIR with $$ \hat{p}(x) = L_e(x)cos(\theta)\frac{cos(\theta_L)}{|P_c - x|^2}$$ 
is between 1.5x and 2x slower than power sampling, depending on the scene so power sampling 2SPP corresponds roughly to an equal-time comparison with ReGIR and that $$\hat{p}(x)$$.

As expected, adding $$cos(\theta)$$ to the target function helps a lot. It's most visible on the lampshade at the ceiling in the white room where samples to the light
inside the lampshade (which were the samples contributing the most with $$\hat{p(x)} = \frac{L_e}{d^2}$$) are now discarded because they are below the surface. In other
scenes, no such extreme case so the improvement from $$cos(\theta)$$ is more modest.

Adding $$cos(\theta_L)$$ helps even more than $$cos(\theta)$$ because my renderer does not allow backfacing lights. The result is that around half the lights of these scenes
are discarded from the onset, during grid fill when the target function contains that $$cos(\theta_L)$$ term. We end up with samples that are distributed much more often on lights
that are not backfacing to the shading points, that is, lights that actually have a non-zero contribution.

Adding visibility also reduces variance massively in all scenes but the white room is the one that benefits the most from it. However, the renders with visibility in the target function
look a bit blocky, artifacty, not clean, not uniform, ... It's not that delectable. The major issue I think is that:

- Visibility is quite a sensitive term: it completely zeroes out the target function if the sample is occluded. However, during grid fill, we're computing visibility between the representative point
of the grid cell $$P_c$$ and the point on the light $$x$$. Always using $$P_c$$ here is a problem because that point does not represent the grid cell very well in most situations. The visibility term
for a given light may vary significantly within a grid cell so computing the visibility of the samples only, and always, from $$P_c$$ isn't great and this results in grid cell artifacts where noise increases
significantly where that $$P_c$$-visibility assumption turns out to be wrong: light samples will never be selected by the grid fill even though they have a non-zero contribution at some points in the grid cell other
than $$P_c$$ (this is a source of bias for those wondering, and the subject is tackled in a following section).

The recap of that visibility-term precision is that we should have more precision where lighting frequency is high: if visibility changes faster than the precision of the hash grid, then that's where we
run into issues. More on that in the section on potential improvements.

But when visibility is accurate, it looks quite good: the walls on the left and right, the ceiling and the floor: flat surfaces where there is little risk of making visibility-term mistakes.

<style>
.table_component {
    overflow: auto;
    width: 100%;
}

.table_component table {
    border: 1px solid #dededf;
    height: 100%;
    width: 100%;
    table-layout: fixed;
    border-collapse: collapse;
    border-spacing: 1px;
    text-align: center;
}

.table_component caption {
    caption-side: top;
    text-align: center;
}

.table_component th {
    border: 1px solid #dededf;
    background-color: #eceff1;
    color: #000000;
    text-align: center;
    padding: 5px;
}

.table_component td {
    border: 1px solid #dededf;
    background-color: #ffffff;
    color: #000000;
    padding: 5px;
}
</style>
<div class="table_component" role="region" tabindex="0">
<table>
    <caption>Rendering efficiency new target function</caption>
    <thead>
        <tr>
            <th><br></th>
            <th><br></th>
            <th><br></th>
            <th>Time to converge</th>
            <th><br></th>
            <th>Power sampling only</th>
            <th>TOOOOOOOOOOOOODDDDDDOOOOOOOO WE WANT LIGHT TREE HERE</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Target function</td>
            <td>Le/d^2</td>
            <td>+cos(theta)</td>
            <td>+cos(theta_L)</td>
            <td>+V</td>
            <td><br></td>
            <td><br></td>
        </tr>
        <tr>
            <td><b>The white room (75%)</b></td>
            <td>13.48s</td>
            <td>11.3s</td>
            <td>8.46s</td>
            <td><b>5.38s</b></td>
            <td>6.252s</td>
            <td><br></td>
        </tr>
        <tr>
            <td><b>Bistro (50%)</b></td>
            <td>12.36s</td>
            <td>9.1s</td>
            <td><b>5.86s</b></td>
            <td>33.82s</td>
            <td>63.78s</td>
            <td><br></td>
        </tr>
        <tr>
            <td><b>BZD measure-seven&nbsp;</b><b>(75%)</b></td>
            <td>9.24s</td>
            <td>7.68s</td>
            <td><b>5.6s</b></td>
            <td>31.9s</td>
            <td>46.88s</td>
            <td></td>
        </tr>
		<tr>
            <td><br></td>
            <td><br></td>
            <td><br></td>
            <td><b></b></td>
            <td><br></td>
            <td><br></td>
            <td><br></td>
        </tr>
        <tr>
            <td><br></td>
            <td><br></td>
            <td><br></td>
            <td><b>Samples to converge</b></td>
            <td><br></td>
            <td><br></td>
            <td><br></td>
        </tr>
        <tr>
            <td><b>The white room&nbsp;</b><b>(75%)</b></td>
            <td>4690</td>
            <td>3842</td>
            <td>2823</td>
            <td><b>387</b></td>
            <td>3141</td>
            <td><br></td>
        </tr>
        <tr>
            <td><b>Bistro&nbsp;</b><b>(50%)</b></td>
            <td>2319</td>
            <td>1626</td>
            <td>1001</td>
            <td><b>810</b></td>
            <td>20781</td>
            <td><br></td>
        </tr>
        <tr>
            <td><b>BZD measure-seven&nbsp;</b><b>(75%)</b></td>
            <td>2545</td>
            <td>2021</td>
            <td>1450</td>
            <td><b>1391</b></td>
            <td>17086</td>
            <td></td>
        </tr>
        <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
        </tr>
    </tbody>
</table>
</div>

Although visibility looks quite good in terms of variance (omitting the artifacts which we'll address very soon), it is clearly isn't very efficient in general. It performs well
in the white room because this is a scene that has complicated visibility (despite looking like a simple scene) and that is also fairly fast to trace. In the two other scenes however,
it is clearly way too expensive to be worth it in terms of rendering efficiency (but it does improve quality as we can see by the lower sample counts needed to converge to the same
quality).

Before having a closer look at what we can do for visibility sampling, let's address the artifacts.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/hash-no-normals-artifacts.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Grid cell artifacts because of surface mismatch between the grid cell surface normal N_c and the shading point's surface normal. Largely amplified by the visibility term.
</div>

For the case of the couch, the issue is that the point $$P_c$$ and surface normal $$N_c$$ (in green) used by that grid cell (the ones that were inserted at the creation of the grid cell)
are only on one side of the couch. But during NEE, we may have shading points on both "sides of the couch". This creates a situation where some shading points (in red) are using ReGIR samples
that were produced with a grid cell that has geometric information (and thus grid fill target function) completely different from the geometry at that shading point. When this happens, we get bias
in the form of darkening (or bad variance instead of bias if the ReGIR implementation is unbiased, as explained later).

> ##### Note on bias
>
> In practice, when some shading points are not able to find NEE samples at path tracing time because of large differences between the grid fill target function
> and the surface at the shading point, we get large amounts of bias in the form of darkening. This is because some lights will never be able to be sampled from those shading
> points because the grid fill target function is 0 for those light samples, even though they would have had non-0 contribution if evaluated by NEE for that shading point.
> The reason that the screenshots above manifest large amounts of noise in those regions instead of darkening is because the implementation is unbiased and compensates for
> those missing samples, as explained in a later section.
{: .block-warning }

And easy solution to fix this is to introduce the surface normal in the hash grid function:

```c++
unsigned int hash_quantize_normal(float3 normal, unsigned int precision)
{
    unsigned int x = static_cast<unsigned int>(normal.x * precision) << (2 * precision);
    unsigned int y = static_cast<unsigned int>(normal.y * precision) << (1 * precision);
    unsigned int z = static_cast<unsigned int>(normal.z * precision);

    return x | y | z;
}

unsigned int custom_regir_hash(float3 world_position, float cell_size, unsigned int total_number_of_cells, unsigned int& out_checksum) const
{
	// The division by the cell_size gives us control over the precision of the grid. Without that, each grid cell would be size 1^3
	// in world-space
	unsigned int grid_coord_x = static_cast<int>(floorf(world_position.x / cell_size));
	unsigned int grid_coord_y = static_cast<int>(floorf(world_position.y / cell_size));
	unsigned int grid_coord_z = static_cast<int>(floorf(world_position.z / cell_size));

	unsigned int quantized_normal = hash_quantize_normal(surface_normal, primary_hit ? ReGIR_HashGridHashSurfaceNormalResolutionPrimaryHits : ReGIR_HashGridHashSurfaceNormalResolutionSecondaryHits);
	
	// Using the two hash functions as proposed in [WORLD-SPACE SPATIOTEMPORAL RESERVOIR REUSE FOR RAY-TRACED GLOBAL ILLUMINATION, Boisse, 2021]
	unsigned int checksum = h2_xxhash32(quantized_normal + h2_xxhash32(grid_coord_z + h2_xxhash32(grid_coord_y + h2_xxhash32(grid_coord_x))));
	unsigned int cell_hash = h1_pcg(quantized_normal + h1_pcg(grid_coord_z + h1_pcg(grid_coord_y + h1_pcg(grid_coord_x)))) % total_number_of_cells;

	out_checksum = checksum;
	return cell_hash;
}
```

<div id="regir-hash-grid-with-normals" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-hash-grid-with-normals");
	
	if (dataRegirHashGridWithNormals['imageBoxes'])
		new ImageBox(content, dataRegirHashGridWithNormals['imageBoxes'], 1279, 692);
</script>

The case of the lampshade is a bit different and does not have to do with surface orientation but rather with lighting frequency. If the grid cells are too large, the approximation that
visibility is only computed from $$P_c$$ for the whole grid cell falls appart and we get grid cells artifacts. A proper solution to that would increase grid cell resolution in areas
of high lighting frequency. More about that in "Limitations and potential improvements". In the meantime, something we can do to help with that issue is to jitter the shading points: when evaluating
NEE, jitter the shading point position randomly and only then look up which grid cell that jittered shading point falls into. We then use ReGIR light samples of that grid cell for NEE.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/jittering-grid-cell-artifacts.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Jittering the NEE shading point before running it through the hash function to get the grid cell reduces grid cells artifacts, not just in high lighting frequency areas.
</div>

Grid cells artifacts are massively reduced and the image looks much cleaner.

Jittering the NEE shading point's position completely randomly (i.e. with random vec3()) isn't really a good idea for one main reason: the jittered point may end up outside of the scene.
For a shading point on a wall of the white room for example, jittering in the direction normal to the wall will move the point either outside of the room (point jittered in the direction opposite
to the normal of the wall) or into the empty space of the room (point jiterred in the direction of the normal of the wall). In any case, this is a risk of having no grid cell (and thus light samples) to fetch
at that jittered position, which is biased. One solution is to jitter the point again (and even up to a maximum of N times) if the first jittering failed (no grid cell was found at the jittered position), hoping that
a second jittering will jitter correctly this time. This is however expensive as each jittered point needs to be hashed and looked up into the hash grid, with collision resolution and everything that the
implementation entails. The better solution that I use is to always jitter the point in the tangent plane of the surface normal. With the example of the wall in the white room, this would jitter the point exactly
in the direction of the wall and thus not miss the surface of the scene. This optimization helps a bit with performance as less jitter retries as necessary to find a valid jittered position.

```c++
float3 jitter_world_position_tangent_plane(float3 original_world_position, float3 surface_normal, Xorshift32Generator& rng, float grid_cell_size, float jittering_radius = 0.5f)
{
	// Getting the tangent plane vectors from the normal. The ONB can be built
	// with the method of your choice.
	float3 T, B;
	build_ONB(surface_normal, T, B);

	// Offsets X and Y in the tangent plane
	float random_offset_x = rng() * 2.0f - 1.0f;
	float random_offset_y = rng() * 2.0f - 1.0f;

	// Scaling by the grid size
	float scaling = grid_cell_size * jittering_radius;
	random_offset_x *= scaling;
	random_offset_y *= scaling;

	return original_world_position + random_offset_x * T + random_offset_y * B;
}
```

Both techniques can fail however and never find a good jittered point, in which case, to avoid bias (or variance if the implementation is unbiased), my implementation falls back to using the non-jittered original shading point for looking up
the grid cell and fetching the light samples.
	
## NEE++

## BSDF resampling
		- Better product sampling options out there
		- Doesn't work at secondary hits
		- Roughness in hash function

## Reducing grid artifacts
		
## Spatial reuse

## Temporal reuse

Not used because of correlations

## Fixing the bias
	- With cosine terms and everything in the grid fill target function, we now have an issue because some lights may be rejected -> need canonical samples
		- We need good MIS weights so that canonical sample don't add too much noise
	- Bias on perfect mirror surfaces: we need BSDF samples MIS at shading time
		- MIS: New multi-pairwise MIS for shading

## Multiple Importance Sampling
	- MIS also helps for weighting canonical and non canonical candidates
	- Multi pairwise MIS

## Reducing correlations
	- Correlation reduction
	- More reservoirs per cell
	- Better initial samples

## Improving the base samples
	- Cache cells
	- Variance issue when we have a ton of lights contributing to the same cell but our alias table can only contain so many lights so the rest has to be sample with a fallback sampling technique which adds a ton of noise
	
## VRAM optimizations for cache cells

## Improvements to visibility noise

# ReSTIR x ReGIR

# Performance and memory usage

# Limitations and potential improvements
	Mention that visibility term precision issue with lighting frequency being higher frequency thatn the grid, we need some adaptive grid precision or something.

## Correlations
	- More reservoirs per cell
	- Using reservoirs of past frames to improve the effective reservoir count at frame N
		- Doesn't work when accumulating because we now have temporal correlations in between frames
		
## Choosing the grid size
	- Improving cache plkaceemnt for...
	- Envmap sampling integration
	- Antithetic sampling inspired by...

## Determinism
	- Should be a way around it, just haven't got to it yet
