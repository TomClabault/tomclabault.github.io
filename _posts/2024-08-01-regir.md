---
layout: distill
title: ReGIR - An advanced implementation for many-lights offline rendering
date: 2025-08-17 14:45:00+0200
description: 
tags: path-tracing
thumbnail: assets/img/blogs/regir/thumbnail.jpg
categories: hiprt-path-tracer
related_posts: false
related_publications: true
bibliography: blogs/regir-many-lights.bib
---





<!-- Scripts for the ImageBox -->
<link rel="stylesheet" href="/assets/css/distill-width-override.css">
<link rel="stylesheet" href="/assets/css/ImageBox/ImageBox.css">
<script src="/assets/js/ImageBox/ImageBox.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataRISNoVisibility.js"></script>
<!-- Scripts for the ImageBox -->













## The many-lights sampling problem

To render a 3D scene, a naive "brute-force" path tracer sends out rays from the camera, bounce the rays around the scene multiple
times and hopes fingers crossed that the ray is eventually going to hit a light. In the event that the ray leaves the scene without having
hit a light, the pixel stays black and you've done the work for nothing. On top of that if the lights of your scene are very small or very far
away (or more generally, are small in solid angle at your shading point), then naively bouncing rays around is going to have a very low
probability to actually hit a light source.

The solution to that in practice, rather than hoping that the rays are going to hit a light, is to purposefully choose a light from all the lights
that exist in your scene, shoot a shadow ray towards that light to see if it is occluded, and if it's not, use that light to shade the current vertex
along your path. This method is called next-event estimation (NEE).

However, it isn't that simple. The subtle question is: how do I choose the light that I shoot a shadow ray to? Do we just choose the light completely at random?
That works, but from an efficiency standpoint, this is far from ideal, especially when there are tons of lights in the scene: 
- I want to choose a light for my point $X$ on a surface
- There are 1000 lights in the scene
- Only 1 light meaningfully illuminates my point $X$, all the other 999 lights are too far away and don't contribute much at all to $X$

If I choose the lights for NEE completely at random, there's a 1/1000 chance that I'm going to choose the good light for the point $X$. That's not amazing and
this is going to cause very high variance (i.e. very high levels of noise) in our image and our render is going to take a while to converge to a clean image.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/twr-uniform-1SPP-vs-ref.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: one sample per pixel, choosing lights completely at random for NEE.<br>
	Bottom: reference, converged image.<br>
	This scene contains ~137k emissive triangles.
</div>

Surely this is going to take a while to converge.

> ##### Note
>
> Unless stated otherwise, all images in this blog post are rendered with 0 bounces (shoot a camera ray, find the first hit, do next-event estimation and you're done). 
> This is to focus on the variance of our NEE estimator. Adding more bounces would introduce the variance of path sampling and it would be harder to
> evaluate whether we're going in the right direction or not with regards to reducing the variance of our NEE estimator.
{: .block-tip }

Brief detour to the mathematics of why we have so much noise when choosing lights uniformly at random.

Ultimately, when estimating direct lighting using next-event estimation, we're trying to solve the following integral at each point $P$ in our scene:

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{}\rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}dA_x
\label{directLightingIntegral}
\end{equation}

With:
- $L_o(P, \omega_o)$ the reflected radiance of point $P$ in direction $\omega_o$, the **outgoing light** direction (direction towards the camera for example)
- $\int_{A}$ is the integral over the surface of our lights in the scene: to estimate the radiance that our point $P$ receives from the lights of the scene, we need a way of taking into account
all the lights of the scene. One way of doing that is to take all the lights and then consider all the points on the surface of these lights.
"Considering all the points of the surfaces of all the lights" is what we're doing here when integrating over $A$. $A$ can then be thought of as the union of
the area of all the lights and we're taking a point $dA_x$ on this union of areas.
- $\rho(P, \omega_o, \omega_i)$ is the BSDF used to evaluate the reflected radiance at point $P$ in direction $\omega_o$ of the **ingoing light** direction $\omega_i$ (which is $x - P$).
- $L_e(x)$ is the emission intensity of the point $x$
- $V(P\leftrightarrow x)$ is the visibility term that evaluates whether our point $P$ can see the point $x$ on the surface of a light or not (is it occluded by some geometry in the scene).
- $cos(\theta)$ is the attenuation term
- $cos(\theta_L)/d^2$ is the geometry term with $d$ the distance to the point on the light
- $x$ is a point on the surface of a light source

For brevity, we'll use

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{} f(x)dx
\label{directLightingIntegralBrevity}
\end{equation}

with

\begin{equation}
f(x) = \rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{directLightingIntegralBrevityFx}
\end{equation}

In a path tracer, we usually compute the value of this integral with monte carlo integration:
- Pick a sample (a point on a light) $x$ with some probability distribution function (PDF) $p(x)$
- Evaluate $\frac{f(x)}{p(x)}$

The average value of many of those samples will get closer and closer to the true value of equation **(\ref{directLightingIntegral})** which is the value that we want to compute.

Ideally, our PDF $p(x)$ is proportional to f(x): $p(x) = c * f(x)$. If this is the case, then every value $\frac{f(x)}{p(x)}$ that we compute is going to be the constant $c$:

\begin{equation}
\frac{f(x)}{p(x)} = \frac{f(x)}{c * f(x)} = c
\label{ZeroVarianceEq}
\end{equation}

In other words, no matter what random sample $x$ we evaluate, we will always get the same value $c$ out. This means that our estimator is going to have 0 variance.
And with a zero-variance-estimator we get no noise in the image (the noise that we see is just a consequence of neighboring pixels having quite a different value because of variance, even 
though nearby pixels on the image should be probably be almost the exact same color) and the whole image converges with a single sample per pixel. Very yummy.

So the ultimate goal is to be able to sample points on lights with a distribution $p(x)$ that follows $f(x)$ **(Eq. \ref{directLightingIntegralBrevityFx})**. This means, in order of the terms of $f(x)$:

1. $\rho(P, \omega_o, \omega_i)$: we want our sample $x$ to follow the shape of the BSDF: don't sample a point on a light that results in a $\omega_i$ that is outside of the delta peak of a specular BSDF for example
2. $L_e(x)$: sample lights proportionally to their power, the more powerful the more chance the light should have to be sampled
3. $V(P\leftrightarrow x)$: sample only visible points; don't sample a point on a light that is occluded from the point of view of $P$
4. $cos(\theta)$: sample light directions while accounting for the cosine-weighted falloff at the shading point $P$; directions closer to the surface normal contribute more than grazing ones
5. $\frac{cos(\theta_L)}{d^2}$: favor light samples that are both closer to $P$ (stronger due to distance attenuation) and oriented toward $P$ (the lightâ€™s surface normal aligned with direction $\omega_i$)

Which of those terms do we take into account when choosing a light completely at random for NEE? None of those. Hence why it's so bad.

A much better technique already is to sample lights according to their power: before rendering starts, compute the power of all the lights of the scene and build a CDF, an alias table (or anything that allows sampling
a CDF) on that list of light power. Lights can then be sampled exactly proportional to their power, meaning that we've gained the second term, $L_e(x)$.

The results are already much better than uniform sampling:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/twr-uniform-1SPP-vs-power-vs-ref.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: same as before: uniform sampling.<br>
	Center: power proportional sampling<br>
	Bottom: reference, converged image<br>
	This scene contains ~137k emissive triangles.
</div>

This is still far from perfect obviously. 3 areas area of particular interest:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/zoom-above-lamp-indexed.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Inset on the lamp
</div>

The first one is above (and below) the lamp as shown in the screenshot above. Looking at the reference image, the lamp inside the lampshade should clearly illuminate
the wall behind it, above and below the lampshade but we have almost nothing here. The reason for that is the same as for the chimney below: lights are not sampled based on
their distance ($1/d^2$ in term 5. $\frac{cos(\theta_L)}{d^2}$) to the shading point. Relative to the whole scene, the lights inside the lampshade and inside the chimney are not really powerful. The consequence of that is
that our power-proportional sampling does not favor those lights very much (because other lights are more powerful) even though those are the lights contributing the most to the
shading points above and below the lampshade and inside the chimney respectively. Those are the lights that our sampling strategy should have sampled here.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/zoom-chimney-indexed.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Inset on the chimney. The lack of spatial-aware sampling (and visibility sampling) is the cause for the chimney looking almost all black at 1SPP.
</div>

For the case of the chimey, the lack of information about visibility (term 3.$V(P\leftrightarrow x)$) is also an issue. Power sampling alone would not be too bad of an idea
if the large light panels in the front and the back of the room were visible from inside the chimney, but that's not the case. Those large light panels
are sampled often even though they are occluded from the inside of the chimney: our sampling distribution is far from proportional to our actual integrand $f(x)$
and we suffer from lots of noise because of that (we only have term 2. $L_e(x)$ after all).

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/back-of-couch.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: inset on the back of the couch <br>
	Bottom: Another point of view of the scene. The large quad light in the back is the only light that is able to light to back of the couch.
</div>

The case of the back of the couch is also interesting. Looking at the different point of view of the scene, we can see that the only light that is illuminating the back of the couch
is the large light panel in the back of the room. All the other lights of the scene are behind the back of the couch (they are in the back of the back of the couch.... ** visible confusion **).
This is a case where sampling according to term 4. $cos(\theta)$ would help immensely because that term here would be negative for all lights except the large light panel that matters to us.

The BSDF term 1. $\rho(P, \omega_o, \omega_i)$ will be tackled later. For the ones screaming "light hierarchies" in the back, we'll have a look at those later too.

## Resampled Importance Sampling (RIS)

Let's now introduce Resampled Importance Sampling (RIS).

RIS<d-cite key="RISTalbot"></d-cite> is at the heart of ReGIR. At its core, it is a technique that takes multiple samples $(X_1, ..., X_M)$ produced according to a **source distribution $p$** and "resamples" them
into one single sample $Y$ that follows a distribution $\overline{p}$ proportional to a **given (user-defined) target function $\hat{p}$**. Said otherwise, RIS takes a bunch of samples $(X_1, ..., X_M)$ (which can
be produced by our simple uniform sampling or power sampling routine from before for example) and outputs one new sample $Y$ that is distributed closer to whatever function $\hat{p}$ we want (to be precise the sample $Y$
will follow $\overline{p}$ closer, not $\hat{p}$ which may not be normalized and thus may not be a PDF).

Let's quickly dive into a concrete example applied to our many-light sampling problem before this gets all too theoretical:

Recall that the integral that we want to solve at each point $P$ in our scene is:

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{}\rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}dA_x
\label{directLightingIntegralRIS}
\end{equation}

Now again, what a naive path tracer does to solve this integral is: at each point $P$ in the scene, choose a light $l$ randomly (uniformly, proportional to power, light hierarchies for the ones in the back, ...),
choose a point $x$ on the surface of this light and evaluate equation **(\ref{directLightingIntegralRIS})**.

With the lights chosen uniformly, the probability of chosing the light $l$ is $\frac{1}{\| Lights\|}$ and the probability of uniformly choosing the point $x$ on the surface of that light $l$ is $\frac{1}{Area(l)}$
with $\|Lights\|$ the number of lights in the scene. 

Our **source distribution $p$** is then defined as:

\begin{equation}
p(x) = \frac{1}{\|Lights\| * Area(l)}
\label{sourceDistributionP}
\end{equation}

That explains the first part of the sentence about RIS: "it is a technique that takes multiple samples $(X_1, ..., X_M)$ produced according to a **source distribution $p$**". We've got our source distribution $p$
and we can generate samples (points on light sources) with it. 

The next step is then to use RIS to "resample them into one sample that follows a distribution $\overline{p}$ proportional to a **given (user-defined) target function $\hat{p}$**."

So first of all, what's going to be our user-defined target function $\hat{p}$? Why not choose equation **(\ref{directLightingIntegralRIS})** to be our target function so that RIS outputs
samples that are proportional exactly to the function that we're trying to integrate (which is ultimately the goal)? We can totally do that:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-4k-samples.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	1SPP with RIS, resampling 4000 power-proportional light samples $(X_1, ..., X_{4000})$ with the full $f(x)$ as the target function $\hat{p}$
</div>

Delectable.

This runs at 0.5FPS. 

Not so delectable anymore. 

In effect, doing this will not be efficient, mostly because of the visibility term that requires tracing a ray, which is an expensive operation. In practice, the target function $\hat{p}$ needs to be cheap-ish to evaluate or
RIS will turn out to be computationally inefficient. This worked okay for this scene because it's not too expensive to trace. And because power-proportional sampling does not do a catastrophically bad job
at sampling the scene to begin with. The same 4000-samples-RIS NEE strategy but using uniform sampling this time isn't as shiny:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-4k-uniform-samples.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	1SPP with RIS, resampling 4000 uniform ight samples $(X_1, ..., X_{4000})$. Bad. <br>
	This show the importance of the "base sampling strategy" that the samples we feed RIS are produced with. If our **source distribution $p$** is very bad, RIS will have a hard time getting a good sample out of it.
</div>

So if the full equation **(\ref{directLightingIntegralRIS})** is too expensive because of the visibility term, let's define our target function $\hat{p}$ without that visibility term:

\begin{equation}
\hat{p}(x) = \rho(P, \omega_o, \omega_i)L_e(x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{targetFunctionPHatNoVis}
\end{equation}

This
is what such a target function can give us:

<div id="ImageBoxContent" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("ImageBoxContent");
	
	if (dataRISNoVisibility['imageBoxes'])
		new ImageBox(content, dataRISNoVisibility['imageBoxes'], 1279, 692);


	// if (dataRISNoVisibility['stats'])
		//new ChartBox(content, dataRISNoVisibility['stats']);
		//new TableBox(content, dataRISNoVisibility['stats']);
</script>

There are a couple of interesting things happening here with that $\hat{p}$ from equation **(\ref{targetFunctionPHatNoVis})**:
1. The 40000 samples RIS render doesn't look that much better than the 1000 one
2. The 40000 samples RIS render looks worse in some places than the 10 samples RIS render (mostly around the lampshade)

For 1., the likely explanation is that at M=1000 samples, the sample $Y$ output by RIS is already distributed pretty much perfectly 
(almost perfectly. Looking closer at the back of the couch you see that this is still a bit noisy. The noise here comes from term 5. $\frac{cos(\theta_L)}{d^2}$ of equation **(\ref{directLightingIntegralBrevityFx})**)
according to $\hat{p}(x) = \rho(P, \omega_o, \omega_i)L_e(x)cos(\theta)\frac{cos(\theta_L)}{d^2}$.
Adding more samples won't make a difference since the sample is already perfect. It's a perfect sample but only with regards to that $\hat{p}$ without the visibility term $V$. 
Because we omitted that visibility term, the distribution according to which our samples $Y$ are produced is still not completely proportional to our ultimate goal, equation **(\ref{directLightingIntegralBrevityFx})**:

\begin{equation}
f(x) = \rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{directLightingIntegralBrevityFxAgain}
\end{equation}

That lack of proportionality to the visibility is the cause for all the rest of the noise visible in the image. Who would have guessed that visibility was important...

For 2., looking closer at the area around the green point on the left of the image, it's noticeable that RIS with 10 candidates (M = 10) looks a bit better than M = 40000:

<div class="col-sm mt-3 mt-md-0">
	{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-no-vis-inset-10-10000-green-spot.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: M = 10 <br>
	Bottom: M = 40000
</div>

This is because at M = 40000, our sample $Y$ is distributed much closer to $\overline{p}$ 
(remember that $\hat{p}(x)$ isn't necessarily a PDF, it's not necessarily normalized. So strictly speaking, the samples $Y$ are distributed closer to the normalized version of $\hat{p}(x)$, which is $\overline{p}$)
than with M = 10. However, this does not play in our favor here because of the missing visibility term. In this exact instance, getting closer to $\hat{p}$ without the visibility term $V$ (equation **(\ref{targetFunctionPHatNoVis})**) gets us
further away from our goal, equation **(\ref{directLightingIntegralBrevityFxAgain})**:

<div class="col-sm mt-3 mt-md-0">
	{% include figure.liquid path="assets/img/blogs/regir-many-lights/RIS-curves.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	RIS produces samples $Y$ that are closer and closer to the target distribution $\overline{\hat{p}}$ (pHatBar, our target function $\hat{p}$ without visibility, but normalized) as $M$
	increases but this also moves us further away from the perfect distribution $pf(x)$ ($f(x)$ but normalized such that it is a PDF) <br>
	
	<br>
	The X axis represents all possible light samples in the scene. <br>
	The Y axis is the probability that a given distribution produces that light sample <br>
	<br>
	
	These curves are approximate, they are just to illustrate the idea.
</div>

The curves above show us that increasing M gets us closer to $\overline{\hat{p}}$ but in some places, this also moves us further away from the true goal which is distributing our samples $Y$ proportional to $f(x)$, as we can see
with the dip in $pf(x)$ on the right of the graph. That dip is caused by the light in the lampshade being occluded from the point of view of the green point. RIS will keep choosing that light because it's close to our
green point but it is occluded so most samples will end up having 0 contribution in the end and this is the cause for the variance here.

Now to get back to our numerical example of how to actually use RIS, what do we do with our target function $\hat{p}$? We're going to use it to "resample" our samples $(X_1, ..., X_M)$ produced from $p$.

Here's the pseudocode of the RIS algorithm:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/restir-di/RISAlgo.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Resampled Importance Sampling (RIS) pseudo-code algorithm. Source: "A Gentle Introduction to ReSTIR", 2023<d-cite key="Wyman2023Gentle"></d-cite>
</div>

Algorithm starts at line 10. The idea is simple.

- (Line 10-11) Generate our $M$ candidates according to the distribution $p$ we talked about earlier. This corresponds to picking points on the surface of the lights of our scene.
- (Line 12) Compute the "resampling weight" $w_i$ of this candidate $X_i$. 
	- $m_i$ is a MIS weight, will be introduced in the next sections. For now, we'll use $m_i=\frac{1}{M}$
	- $\hat{p}(X_i)$ is the value of the target function for the generated sample $X_i$
	- $W_{X_i}$ is what we call the "unbiased contribution weight" of the sample/candidate. This term will be thoroughly detailed in the next sections.
	For now, we'll use $W_{X_i}=p(X_i)$, the probability of picking *that* point $X_i$ on *the* light in the scene the point belongs to.
- (Line 14) With these weights $w_i$, we are now going to choose a sample $X_i$ proportionally to its weight $w_i$: 
	- Let's say we have $M=3$ candidates. We compute their resampling weights $w_i$ and obtain $(w_1, w_2, w_3)=(1.2, 2.5, 0.5)$. The *randomIndex()* function can then be
	used to obtain the sample $X_i$ (it uses a non-precomputed CDF inversion technique for choosing that sample <d-cite key="PBRBookSampling1DFunction"></d-cite>).
	Concretely in this example, the probability of choosing $X_1$, $X_2$ or $X_3$ with the random index $s$ is then:
	
$$ p(s=0)=\frac{1.2}{1.2+2.5+0.5}=0.2857 $$

$$ p(s=1)=\frac{2.5}{1.2+2.5+0.5}=0.595 $$

$$ p(s=2)=\frac{2.5}{1.2+2.5+0.5}=0.119 $$

With the random index $s$ chosen, we can retrieve the final sample $Y$: the output of RIS. This sample $Y=X_s$ represents a combination of the $M$ samples that
we input to RIS such that this sample $Y$ (and its weight $W_Y$ which we'll get to in a moment) follows a distribution $\overline{p}$ (which we call the "target PDF")
proportional to the "target function" $\hat{p}$ that we chose. 

> ##### **Target PDF vs. Target Function**
>
> Note that "target PDF" and "target function" are two different concepts. The first ReSTIR paper<d-cite key="bitterli2020spatiotemporal"></d-cite>
used the term "target PDF" in place of "target function" but we know since then<d-cite key="Lin2022"></d-cite> that this was a slight semantic mistake:
the target function $\hat{p}$ is given by the user (that's you) and may not be normalized. The $Y$ sample produced, however, follows the normalized
distribution $\overline{p}$ of $\hat{p}$ which is normalized (and thus adequate for use as a PDF) and that is the "target PDF".
{: .block-tip }

Now, in Monte Carlo integration, we need the probability (PDF) of sampling a given sample. This also applies to next event estimation as you may know: when you
sample a light in the scene, you divide by the probability of sampling that light when evaluating the radiance contribution of that light at the point you're shading.

RIS is no exception to that. RIS returns a light sample $Y$ and its weight $W_Y$ from the many $M$ light samples that we fed it. If we want to use that light
sample $Y$ for evaluating direct lighting contribution, we're going to need its PDF. The hard truth is that RIS "mixes $M$ samples together" and as a result,
we can't<d-cite key="bitterli2020spatiotemporal"></d-cite> easily compute the PDF of the resulting sample $Y$. What we can compute however, is an estimate of
its PDF. That's what the $W_Y$ computed at line 17 of the algorithm is. This estimate converges to the inverse PDF $\frac{1}{\overline{p}(Y)}$ of $Y$ as the
number of candidates $M$ resampled increases. If we then want to divide by the PDF of our sample $Y$, we can instead multiply by that estimate of the inverse
PDF $W_Y$ and achieve the same result.

As a matter of fact, a sample output by RIS can be used for evaluation as simply as:

\begin{equation}
f(Y)W_Y
\end{equation}

that is: evaluate the function you want to integrate with the sample $Y$ and multiply by $W_Y$ (the estimate of its inverse PDF $\frac{1}{\overline{p}(Y)}$).

For our direct lighting evaluation case, we can use $Y$ as:

\begin{equation}
L_o(P, \omega_o) = f(P, \omega_o, \omega_i)V(P\leftrightarrow Y)cos(\theta)W_Y
\end{equation}

with $\omega_i$ the direction towards the light sample (point on the surface of a light) $Y$.

One key property of RIS is that the more samples $M$ we feed it, the closer to the target PDF $\overline{p}$ the resulting sample $Y$'s distribution will be. This is the reason why ReSTIR algorithms work at all as we'll see.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/restir-di/RISMValues.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Sampling the distribution represented by $M=\infinity$ with candidates $(X_1, ..., X_M)$ uniformly distributed. As $M$ increases and we resample more and more candidates through RIS, the output candidates are distributed closer and closer to the target distribution. Illustration from [Talbot, 2005]<d-cite key="RISTalbot"></d-cite>
</div>

> ##### **Why is the $Y$ sample from RIS better than $X_s$?**
>
> If RIS takes 5 samples as input and returns only 1 of the 5 as output, how is that an improvement? The key difference is that the sample $Y$ 
(which is 1 of the 5 that we fed RIS) now has a weight $W_Y$ associated with it. That weight makes all the difference as it accounts for all the
samples that RIS has seen and makes it so that the sample $Y$ isn't exactly $X_s$ with PDF $\ W_{X_s}=p(X_s)$ anymore, it is a whole new sample with an
UCW $\ W_Y$ (inverse PDF estimate) that is distributed closer to $\overline{p}$ (defined by our given $\hat{p}$) than any of the 5 samples alone.
{: .block-tip }


One detail that I omitted so far is the unit of the weights $w_i$. Looking at the RIS algorithm above, the $w_i$ weights are float numbers. 
Yet, they are computed with our target function $\hat{p}$ which includes the BSDF term and that BSDF term is a color, not a float. 
In practice, we take the luminance of our target function to compute the $w_i$ weights and so our $w_i$ computation looks more like this:

\begin{equation}
w_i = m_i(X_i)luminance(\hat{p}(X_i))W_{X_i}
\end{equation}

## substitle

citation test<d-cite key="jefferyHierarchicalAdaptiveSampling"></d-cite>

	- We won't be introducing RIS, plenty of resources to do the job better than me
	- ReGIR basics: RIS per grid cell per reservoir. RIS again at shading time
- Improving ReGIR
	- Representative cell data --> get closer to the integrand with cosine terms and everything
	- Spatial reuse
	- Integration with NEE++
	- Hash grid with surface normals
	- Hash grid with wavy cells to avoid aliasing
	- BRDF resampling at primary hits
		- Better product sampling options out there
		- Doesn't work at secondary hits
- Avoiding bias
	- With cosine terms and everything in the grid fill target function, we now have an issue because some lights may be rejected -> need canonical samples
		- We need good MIS weights so that canonical sample don't add too much noise
	- Bias on perfect mirror surfaces: we need BSDF samples MIS at shading time
		- MIS: New multi-pairwise MIS for shading
- Correlations
	- More reservoirs per cell
	- Using reservoirs of past frames to improve the effective reservoir count at frame N
		- Doesn't work when accumulating because we now have temporal correlations in between frames
- Improving performance
	- Light presampling
	- Less reservoirs at later bounces
- Improving base sampling
	- Cache cells
	- Variance issue when we have a ton of lights contributing to the same cell but our alias table can only contain so many lights so the rest has to be sample with a fallback sampling technique which adds a ton of noise
- A note on adapting that to real-time

