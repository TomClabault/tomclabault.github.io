---
layout: distill
title: ReGIR - An advanced implementation for many-lights offline rendering
date: 2025-08-17 14:45:00+0200
description: 
tags: path-tracing
thumbnail: assets/img/blogs/regir/thumbnail.jpg
categories: hiprt-path-tracer
related_posts: false
related_publications: true
bibliography: blogs/regir-many-lights.bib
---





<!-- Scripts for the ImageBox -->
<link rel="stylesheet" href="/assets/css/distill-width-override.css">
<link rel="stylesheet" href="/assets/css/ImageBox/ImageBox.css">
<script src="/assets/js/ImageBox/ImageBox.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataRISNoVisibility.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataRegirLightSamplesPerReservoir.js"></script>
<script src="/assets/blogs-assets/ReGIR-Many-Lights/ImageBox/dataHashGridCollisions.js"></script>
<!-- Scripts for the ImageBox -->






# The many-lights sampling problem

# Resampled Importance Sampling (RIS)



# ReGIR : Reservoir-based Grid Importance Resampling

## The algorithm

## A better grid

## Improving the target function
	
## NEE++

## BSDF resampling
		
## Spatial reuse

## Temporal reuse

## Fixing the bias

## Multiple Importance Sampling

## Reducing correlations

## Improving the base samples
	
## VRAM optimizations for cache cells

## Visibility in the shading target function

## ReSTIRring ReGIR



# Limitations and potential improvements

## Correlations
		
## Choosing the grid size







# The many-lights sampling problem


To render a 3D scene, a naive "brute-force" path tracer sends out rays from the camera, bounce the rays around the scene multiple
times and hopes fingers crossed that the ray is eventually going to hit a light. In the event that the ray leaves the scene without having
hit a light, the pixel stays black and you've done the work for nothing. On top of that if the lights of your scene are very small or very far
away (or more generally, are small in solid angle at your shading point), then naively bouncing rays around is going to have a very low
probability to actually hit a light source.

The solution to that in practice, rather than hoping that the rays are going to hit a light, is to purposefully choose a light from all the lights
that exist in your scene, shoot a shadow ray towards that light to see if it is occluded, and if it's not, use that light to shade the current vertex
along your path. This method is called next-event estimation (NEE).

However, it isn't that simple. The subtle question is: how do I choose the light that I shoot a shadow ray to? Do we just choose the light completely at random?
That works, but from an efficiency standpoint, this is far from ideal, especially when there are tons of lights in the scene: 
- I want to choose a light for my point $X$ on a surface
- There are 1000 lights in the scene
- Only 1 light meaningfully illuminates my point $X$, all the other 999 lights are too far away and don't contribute much at all to $X$

If I choose the lights for NEE completely at random, there's a 1/1000 chance that I'm going to choose the good light for the point $X$. That's not amazing and
this is going to cause very high variance (i.e. very high levels of noise) in our image and our render is going to take a while to converge to a clean image.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/twr-uniform-1SPP-vs-ref.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: one sample per pixel, choosing lights completely at random for NEE.<br>
	Bottom: reference, converged image.<br>
	This scene contains ~137k emissive triangles.
</div>

Surely this is going to take a while to converge.

> ##### Note
>
> Unless stated otherwise, all images in this blog post are rendered with 0 bounces (shoot a camera ray, find the first hit, do next-event estimation and you're done). 
> This is to focus on the variance of our NEE estimator. Adding more bounces would introduce the variance of path sampling and it would be harder to
> evaluate whether we're going in the right direction or not with regards to reducing the variance of our NEE estimator.
{: .block-tip }

Brief detour to the mathematics of why we have so much noise when choosing lights uniformly at random.

Ultimately, when estimating direct lighting using next-event estimation, we're trying to solve the following integral at each point $P$ in our scene:

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{}\rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}dA_x
\label{directLightingIntegral}
\end{equation}

With:
- $L_o(P, \omega_o)$ the reflected radiance of point $P$ in direction $\omega_o$, the **outgoing light** direction (direction towards the camera for example)
- $\int_{A}$ is the integral over the surface of our lights in the scene: to estimate the radiance that our point $P$ receives from the lights of the scene, we need a way of taking into account
all the lights of the scene. One way of doing that is to take all the lights and then consider all the points on the surface of these lights.
"Considering all the points of the surfaces of all the lights" is what we're doing here when integrating over $A$. $A$ can then be thought of as the union of
the area of all the lights and we're taking a point $dA_x$ on this union of areas.
- $\rho(P, \omega_o, \omega_i)$ is the BSDF used to evaluate the reflected radiance at point $P$ in direction $\omega_o$ of the **ingoing light** direction $\omega_i$ (which is $x - P$).
- $L_e(x)$ is the emission intensity of the point $x$
- $V(P\leftrightarrow x)$ is the visibility term that evaluates whether our point $P$ can see the point $x$ on the surface of a light or not (is it occluded by some geometry in the scene).
- $cos(\theta)$ is the attenuation term
- $cos(\theta_L)/d^2$ is the geometry term with $d$ the distance to the point on the light
- $x$ is a point on the surface of a light source

For brevity, we'll use

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{} f(x)dx
\label{directLightingIntegralBrevity}
\end{equation}

with

\begin{equation}
f(x) = \rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{directLightingIntegralBrevityFx}
\end{equation}

In a path tracer, we usually compute the value of this integral with monte carlo integration:
- Pick a sample (a point on a light) $x$ with some probability distribution function (PDF) $p(x)$
- Evaluate $\frac{f(x)}{p(x)}$

The average value of many of those samples will get closer and closer to the true value of equation **(\ref{directLightingIntegral})** which is the value that we want to compute.

Ideally, our PDF $p(x)$ is proportional to f(x): $p(x) = c * f(x)$. If this is the case, then every value $\frac{f(x)}{p(x)}$ that we compute is going to be the constant $c$:

\begin{equation}
\frac{f(x)}{p(x)} = \frac{f(x)}{c * f(x)} = c
\label{ZeroVarianceEq}
\end{equation}

In other words, no matter what random sample $x$ we evaluate, we will always get the same value $c$ out. This means that our estimator is going to have 0 variance.
And with a zero-variance-estimator we get no noise in the image (the noise that we see is just a consequence of neighboring pixels having quite a different value because of variance, even 
though nearby pixels on the image should be probably be almost the exact same color) and the whole image converges with a single sample per pixel. Very yummy.

So the ultimate goal is to be able to sample points on lights with a distribution $p(x)$ that follows $f(x)$ **(Eq. \ref{directLightingIntegralBrevityFx})**. This means, in order of the terms of $f(x)$:

1. $\rho(P, \omega_o, \omega_i)$: we want our sample $x$ to follow the shape of the BSDF: don't sample a point on a light that results in a $\omega_i$ that is outside of the delta peak of a specular BSDF for example
2. $L_e(x)$: sample lights proportionally to their power, the more powerful the more chance the light should have to be sampled
3. $V(P\leftrightarrow x)$: sample only visible points; don't sample a point on a light that is occluded from the point of view of $P$
4. $cos(\theta)$: sample light directions while accounting for the cosine-weighted falloff at the shading point $P$; directions closer to the surface normal contribute more than grazing ones
5. $\frac{cos(\theta_L)}{d^2}$: favor light samples that are both closer to $P$ (stronger due to distance attenuation) and oriented toward $P$ (the light’s surface normal aligned with direction $\omega_i$)

Which of those terms do we take into account when choosing a light completely at random for NEE? None of those. Hence why it's so bad.

A much better technique already is to sample lights according to their power: before rendering starts, compute the power of all the lights of the scene and build a CDF, an alias table (or anything that allows sampling
a CDF) on that list of light power. Lights can then be sampled exactly proportional to their power, meaning that we've gained the second term, $L_e(x)$.

The results are already much better than uniform sampling:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/twr-uniform-1SPP-vs-power-vs-ref.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: same as before: uniform sampling.<br>
	Center: power proportional sampling<br>
	Bottom: reference, converged image<br>
	This scene contains ~137k emissive triangles.
</div>

This is still far from perfect obviously. 3 areas area of particular interest:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/zoom-above-lamp-indexed.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Inset on the lamp
</div>

The first one is above (and below) the lamp as shown in the screenshot above. Looking at the reference image, the lamp inside the lampshade should clearly illuminate
the wall behind it, above and below the lampshade but we have almost nothing here. The reason for that is the same as for the chimney below: lights are not sampled based on
their distance ($1/d^2$ in term 5. $\frac{cos(\theta_L)}{d^2}$) to the shading point. Relative to the whole scene, the lights inside the lampshade and inside the chimney are not really powerful. The consequence of that is
that our power-proportional sampling does not favor those lights very much (because other lights are more powerful) even though those are the lights contributing the most to the
shading points above and below the lampshade and inside the chimney respectively. Those are the lights that our sampling strategy should have sampled here.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/zoom-chimney-indexed.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Inset on the chimney. The lack of spatial-aware sampling (and visibility sampling) is the cause for the chimney looking almost all black at 1SPP.
</div>

For the case of the chimey, the lack of information about visibility (term 3.$V(P\leftrightarrow x)$) is also an issue. Power sampling alone would not be too bad of an idea
if the large light panels in the front and the back of the room were visible from inside the chimney, but that's not the case. Those large light panels
are sampled often even though they are occluded from the inside of the chimney: our sampling distribution is far from proportional to our actual integrand $f(x)$
and we suffer from lots of noise because of that (we only have term 2. $L_e(x)$ after all).

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/back-of-couch.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: inset on the back of the couch <br>
	Bottom: Another point of view of the scene. The large quad light in the back is the only light that is able to light to back of the couch.
</div>

The case of the back of the couch is also interesting. Looking at the different point of view of the scene, we can see that the only light that is illuminating the back of the couch
is the large light panel in the back of the room. All the other lights of the scene are behind the back of the couch (they are in the back of the back of the couch.... ** visible confusion **).
This is a case where sampling according to term 4. $cos(\theta)$ would help immensely because that term here would be negative for all lights except the large light panel that matters to us.

The BSDF term 1. $\rho(P, \omega_o, \omega_i)$ will be tackled later. For the ones screaming "light hierarchies" in the back, we'll have a look at those later too.

# Resampled Importance Sampling (RIS)

Let's now introduce Resampled Importance Sampling (RIS).

RIS<d-cite key="RISTalbot"></d-cite> is at the heart of ReGIR. At its core, it is a technique that takes multiple samples $(X_1, ..., X_M)$ produced according to a **source distribution $p$** and "resamples" them
into one single sample $Y$ that follows a distribution $\overline{p}$ proportional to a **given (user-defined) target function $\hat{p}$**. Said otherwise, RIS takes a bunch of samples $(X_1, ..., X_M)$ (which can
be produced by our simple uniform sampling or power sampling routine from before for example) and outputs one new sample $Y$ that is distributed closer to whatever function $\hat{p}$ we want (to be precise the sample $Y$
will follow $\overline{p}$ closer, not $\hat{p}$ which may not be normalized and thus may not be a PDF).

Let's quickly dive into a concrete example applied to our many-light sampling problem before this gets all too theoretical:

Recall that the integral that we want to solve at each point $P$ in our scene is:

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{}\rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}dA_x
\label{directLightingIntegralRIS}
\end{equation}

Now again, what a naive path tracer does to solve this integral is: at each point $P$ in the scene, choose a light $l$ randomly (uniformly, proportional to power, light hierarchies for the ones in the back, ...),
choose a point $x$ on the surface of this light and evaluate equation **(\ref{directLightingIntegralRIS})**.

With the lights chosen uniformly, the probability of chosing the light $l$ is $\frac{1}{\| Lights\|}$ and the probability of uniformly choosing the point $x$ on the surface of that light $l$ is $\frac{1}{Area(l)}$
with $\|Lights\|$ the number of lights in the scene. 

Our **source distribution $p$** is then defined as:

\begin{equation}
p(x) = \frac{1}{\|Lights\| * Area(l)}
\label{sourceDistributionP}
\end{equation}

That explains the first part of the sentence about RIS: "it is a technique that takes multiple samples $(X_1, ..., X_M)$ produced according to a **source distribution $p$**". We've got our source distribution $p$
and we can generate samples (points on light sources) with it. 

The next step is then to use RIS to "resample them into one sample that follows a distribution $\overline{p}$ proportional to a **given (user-defined) target function $\hat{p}$**."

So first of all, what's going to be our user-defined target function $\hat{p}$? Why not choose equation **(\ref{directLightingIntegralRIS})** to be our target function so that RIS outputs
samples that are proportional exactly to the function that we're trying to integrate (which is ultimately the goal)? We can totally do that:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-4k-samples.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	1SPP with RIS, resampling 4000 power-proportional light samples $(X_1, ..., X_{4000})$ with the full $f(x)$ as the target function $\hat{p}$
</div>

Delectable.

This runs at 0.5FPS. 

Not so delectable anymore. 

In effect, doing this will not be efficient, mostly because of the visibility term that requires tracing a ray, which is an expensive operation. In practice, the target function $\hat{p}$ needs to be cheap-ish to evaluate or
RIS will turn out to be computationally inefficient. This worked okay for this scene because it's not too expensive to trace. And because power-proportional sampling does not do a catastrophically bad job
at sampling the scene to begin with. The same 4000-samples-RIS NEE strategy but using uniform sampling this time isn't as shiny:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-4k-uniform-samples.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	1SPP with RIS, resampling 4000 uniform ight samples $(X_1, ..., X_{4000})$. Bad. <br>
	This show the importance of the "base sampling strategy" that the samples we feed RIS are produced with. If our **source distribution $p$** is very bad, RIS will have a hard time getting a good sample out of it.
</div>

So if the full equation **(\ref{directLightingIntegralRIS})** is too expensive because of the visibility term, let's define our target function $\hat{p}$ without that visibility term:

\begin{equation}
\hat{p}(x) = \rho(P, \omega_o, \omega_i)L_e(x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{targetFunctionPHatNoVis}
\end{equation}

This
is what such a target function can give us:

<div id="ImageBoxContent" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("ImageBoxContent");
	
	if (dataRISNoVisibility['imageBoxes'])
		new ImageBox(content, dataRISNoVisibility['imageBoxes'], 1279, 692);


	// if (dataRISNoVisibility['stats'])
		//new ChartBox(content, dataRISNoVisibility['stats']);
		//new TableBox(content, dataRISNoVisibility['stats']);
</script>

There are a couple of interesting things happening here with that $\hat{p}$ from equation **(\ref{targetFunctionPHatNoVis})**:
1. The 40000 samples RIS render doesn't look that much better than the 1000 one
2. The 40000 samples RIS render looks worse in some places than the 10 samples RIS render (mostly around the lampshade)

For 1., the likely explanation is that at M=1000 samples, the sample $Y$ output by RIS is already distributed pretty much perfectly 
(almost perfectly. Looking closer at the back of the couch you see that this is still a bit noisy. The noise here comes from term 5. $\frac{cos(\theta_L)}{d^2}$ of equation **(\ref{directLightingIntegralBrevityFx})**)
according to $\hat{p}(x) = \rho(P, \omega_o, \omega_i)L_e(x)cos(\theta)\frac{cos(\theta_L)}{d^2}$.
Adding more samples won't make a difference since the sample is already perfect. It's a perfect sample but only with regards to that $\hat{p}$ without the visibility term $V$. 
Because we omitted that visibility term, the distribution according to which our samples $Y$ are produced is still not completely proportional to our ultimate goal, equation **(\ref{directLightingIntegralBrevityFx})**:

\begin{equation}
f(x) = \rho(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)\frac{cos(\theta_L)}{d^2}
\label{directLightingIntegralBrevityFxAgain}
\end{equation}

That lack of proportionality to the visibility is the cause for all the rest of the noise visible in the image. Who would have guessed that visibility was important...

For 2., looking closer at the area around the green point on the left of the image, it's noticeable that RIS with 10 candidates (M = 10) looks a bit better than M = 40000:

<div class="col-sm mt-3 mt-md-0">
	{% include figure.liquid path="assets/img/blogs/regir-many-lights/ris-no-vis-inset-10-10000-green-spot.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Top: M = 10 <br>
	Bottom: M = 40000
</div>

This is because at M = 40000, our sample $Y$ is distributed much closer to $\overline{p}$ 
(remember that $\hat{p}(x)$ isn't necessarily a PDF, it's not necessarily normalized. So strictly speaking, the samples $Y$ are distributed closer to the normalized version of $\hat{p}(x)$, which is $\overline{p}$)
than with M = 10. However, this does not play in our favor here because of the missing visibility term. In this exact instance, getting closer to $\hat{p}$ without the visibility term $V$ (equation **(\ref{targetFunctionPHatNoVis})**) gets us
further away from our goal, equation **(\ref{directLightingIntegralBrevityFxAgain})**:

<div class="col-sm mt-3 mt-md-0">
	{% include figure.liquid path="assets/img/blogs/regir-many-lights/RIS-curves.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	RIS produces samples $Y$ that are closer and closer to the target distribution $\overline{\hat{p}}$ (pHatBar, our target function $\hat{p}$ without visibility, but normalized) as $M$
	increases but this also moves us further away from the perfect distribution $pf(x)$ ($f(x)$ but normalized such that it is a PDF) <br>
	
	<br>
	The X axis represents all possible light samples in the scene. <br>
	The Y axis is the probability that a given distribution produces that light sample <br>
	<br>
	
	These curves are approximate, they are just to illustrate the idea.
</div>

The curves above show us that increasing M gets us closer to $\overline{\hat{p}}$ but in some places, this also moves us further away from the true goal which is distributing our samples $Y$ proportional to $f(x)$, as we can see
with the dip in $pf(x)$ on the right of the graph. That dip is caused by the light in the lampshade being occluded from the point of view of the green point. RIS will keep choosing that light because it's close to our
green point but it is occluded so most samples will end up having 0 contribution in the end and this is the cause for the variance here.

Now to get back to our numerical example of how to actually use RIS, what do we do with our target function $\hat{p}$? We're going to use it to "resample" our samples $(X_1, ..., X_M)$ produced from $p$.

Here's the pseudocode of the RIS algorithm:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/regir-many-lights/RISAlgo.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Resampled Importance Sampling (RIS) pseudo-code algorithm. Source: "A Gentle Introduction to ReSTIR", 2023<d-cite key="Wyman2023Gentle"></d-cite>
</div>

Algorithm starts at line 10. The idea is simple.

- (Line 10-11) Generate our $M$ candidates according to the distribution $p$ we talked about earlier. This corresponds to picking points on the surface of the lights of our scene.
- (Line 12) Compute the "resampling weight" $w_i$ of this candidate $X_i$. 
	- $m_i$ is a MIS weight. For now, we'll use $m_i=\frac{1}{M}$
	- $\hat{p}(X_i)$ is the value of the target function for the generated sample $X_i$
	- $W_{X_i}$ is what's called the "unbiased contribution weight" of the sample/candidate. This term will be thoroughly detailed in the next sections.
	For now, we'll use $W_{X_i}=\frac{1}{p(X_i)}$, the inverse of the probability of picking *that* point $X_i$ on *that* light in the scene the point belongs to.
- (Line 14) With these weights $w_i$, we are now going to choose a sample $X_i$ proportionally to its weight $w_i$: 
	- Let's say we have $M=3$ candidates. We compute their resampling weights $w_i$ and obtain $(w_1, w_2, w_3)=(1.2, 2.5, 0.5)$. The *randomIndex()* function can then be
	used to obtain the sample $X_i$.
	Concretely in this example, the probability of choosing $X_1$, $X_2$ or $X_3$ with the random index $s$ is then:
	
$$ p(s=0)=\frac{1.2}{1.2+2.5+0.5}=0.2857 $$

$$ p(s=1)=\frac{2.5}{1.2+2.5+0.5}=0.595 $$

$$ p(s=2)=\frac{2.5}{1.2+2.5+0.5}=0.119 $$

With the random index $s$ chosen, we can retrieve the final sample $Y$: the output of RIS. This is with this sample $Y$ that we can evaluate NEE.

Now, in Monte Carlo integration, we need the probability (PDF) of sampling a given sample. This also applies to next event estimation as you may know: when you
sample a light in the scene, you divide by the probability of sampling that light when evaluating the radiance contribution of that light at the point you're shading.

RIS is no exception to that. RIS returns one light sample $Y$ and its weight $W_Y$ from the $M$ light samples that we fed it. If we want to use that light
sample $Y$ for evaluating NEE, we're going to need its PDF. The hard truth is that RIS "mixes $M$ samples together" and as a result,
we can't<d-cite key="bitterli2020spatiotemporal"></d-cite> easily compute the PDF of the resulting sample $Y$. What we can compute however, is an estimate of
its PDF. That's what the $W_Y$ computed at line 17 of the algorithm is. This estimate converges to the inverse PDF $\frac{1}{\overline{p}(Y)}$ of $Y$ as the
number of candidates $M$ resampled increases. If we then want to divide by the PDF of our sample $Y$, we can instead multiply by that estimate of the inverse
PDF $W_Y$ and achieve the same result.

As a matter of fact, a sample output by RIS can be used for evaluation as simply as:

\begin{equation}
f(Y)W_Y
\end{equation}

that is: evaluate the function you want to integrate with the sample $Y$ and multiply by $W_Y$ (the estimate of its inverse PDF $\frac{1}{\overline{p}(Y)}$) instead of dividing by $p(x)$
as we would do with non-RIS sampling schemes.

For our direct lighting evaluation case, we can use $Y$ as:

\begin{equation}
L_o(P, \omega_o) = f(P, \omega_o, \omega_i)V(P\leftrightarrow Y)cos(\theta)\frac{cos(\theta_L)}{d^2}W_Y
\end{equation}

with $\omega_i$ the direction towards the light sample (point on the surface of a light) $Y$ from $P$.

So to recap:
- RIS takes $M$ samples as input
- Those M samples are distributed according to a given distribution $p$ that is easy to sample from (power sampling in the screenshots above)
- Evaluate the weights $w_i$ for all these samples
- Choose one sample $Y$ from $(X_1, ..., X_M)$ proportional to its weight
- Compute $W_Y$ for that sample $Y$
- Evaluate NEE with that sample $Y$

And this is done at every shading point in the scene.

# ReGIR : Reservoir-based Grid Importance Resampling

## The algorithm

ReGIR is a light sampling technique built on top of RIS that was first described in the paper by Boksansky et al., 2021<d-cite key="boksansky2021regir"></d-cite>. 
Boissé, 2021<d-cite key="boisse2021worldspaceRestir"></d-cite> also proposed something similar.
The idea is to have a grid (let's assume regular grid for now) built over your scene, and then, in a prepass to the rendering process:
1. For each cell of the grid, consider the point at the center of the grid
2. Sample a few lights, $L = 32$ ($L$ for lights) for example, using a simple sampling technique (power sampling for example)
3. Resample those $L$ lights with RIS by estimating the contribution of each of those $L$ samples to the point at the center of the grid cell
4. This outputs one light sample that is stored in the grid cell
5. Repeat the process a certain number of times, $R = 64$ ($R$ for reservoirs) to get more light samples (reservoirs) in each grid cell
6. At path tracing time, when evaluating NEE, find out which grid cell your shading point falls into
7. Get a few lights from that grid cell, $S = 4$ ($S$ for shading) for example.
8. Resample those $S$ light samples again with RIS
9. This outputs one final light sample
10. Shade that light sample for NEE

Essentially what this accomplishes is that it precomputes a bunch of light samples per each grid cell ahead of time, and then we can use these precomputed light samples
at shading time for NEE, instead of our regular sampling technique.

The good bit here is the target function that we can use in step 2. If we call $P$ the center of our cell, we can use:

\begin{equation}
\hat{p}(x) = \frac{L_e(x)}{|P - x|^2}
\label{regirGridFillTargetFunction}
\end{equation}

that is, a target function that not only contains the power of the light but also the distance to the light. The result is that our grid cells are going to
contain $R$ light samples (or reservoirs as we're using reservoirs to store the output of RIS here, as proposed in the ReGIR paper) that take power and distance into account. 
These power-distance light samples are then used at path tracing time for NEE (by looking up in which grid cell our shading point is) and completely replace the simpler power-proportional samples that we've been using
until now. So now our light sampling technique can take distance into account to produce its samples. Let's see how much of a quality difference that makes:

<div id="regir-light-samples-per-reservoir" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-light-samples-per-reservoir");
	
	if (dataRegirLightSamplesPerReservoir['imageBoxes'])
		new ImageBox(content, dataRegirLightSamplesPerReservoir['imageBoxes'], 1279, 692);
</script>

Quick note: For now, NEE is performed with a single light sample per shading point $S = 1$, as opposed to using RIS as in the introduction to RIS or using $S = 4$ as in step 7. 

Commenting on the white room results first, the behavior is a bit similar to what we had with RIS and the 40000 samples, except it's worse. The lampshade at the ceiling is almost completely black past $L = 16$!
And that's not buggy, the image will converge correctly eventually, that's just very very bad variance. That's because the light samples that we use for NEE are now 
distributed according to the simple target function of equation **(\ref{regirGridFillTargetFunction})**, which misses almost all the terms of our ultimate
goal, equation **(\ref{directLightingIntegralBrevityFx})**. Compared to simple power sampling, we gained the distance term $\frac{1}{d^2}$ but not the other terms and we get into a similar situation
as before (in the introduction to RIS section where we used $\hat{p}$ without the visibility term) where, in some places of the scene, RIS actually moves us further away from the ideal distribution than power sampling is.
That's the case for the exterior of the lampshade where most samples end up sampling the light inside the lampshade but that light is actually behind the surface normal and occluded: all bad samples.

On the Bistro, things look quite a bit better for ReGIR and increasing $L$ results in better and better samples for NEE. That Bistro scene suffers less from that issue of "RIS moving us away from the
ideal target distribution" and ReGIR now significantly outperforms power sampling at equal sample count (just 1SPP in the above screenshots). We'll have equal-time comparisons later.

Yet, those renders are still far from perfect, especially in the white room. So can we do better?

## A better grid

Let's start by addressing the elephant that has been hiding behind the couch: the regular grid.
	
Bad. Mostly because there is a ton of empty space in a regular grid: grid cells that contain no scene geometry at all and that our NEE is never going to query because no ray will ever
hit geometry in those grid cells. This has us filling the grid (computational time) and storing reservoirs in those grid cells (VRAM usage) for nothing. And a lot of them.
	
The better data structure that I ended up using for ReGIR is a hash grid, inspired by the spatial hasing of Binder et al. 2019<d-cite key="binderPSF2019"></d-cite>.
This works by first hashing a "descriptor", getting a hash key out of that hash operation, and using that hash key to index a buffer in memory: that index in the buffer is where we can store
information about the grid cell: the reservoirs of our cells and more.

Probably the most simple and useful descriptor that can be used to start with is just the 3D position in the scene:

```markdown
unsigned int custom_regir_hash(float3 world_position, float cell_size, unsigned int total_number_of_cells, unsigned int& out_checksum) const
{
	// The division by the cell_size gives us control over the precision of the grid. Without that, each grid cell would be size 1^3
	// in world-space
	unsigned int grid_coord_x = static_cast<int>(floorf(world_position.x / cell_size));
	unsigned int grid_coord_y = static_cast<int>(floorf(world_position.y / cell_size));
	unsigned int grid_coord_z = static_cast<int>(floorf(world_position.z / cell_size));

	// Using the two hash functions as proposed in [WORLD-SPACE SPATIOTEMPORAL RESERVOIR REUSE FOR RAY-TRACED GLOBAL ILLUMINATION, Boisse, 2021]
	unsigned int checksum = h2_xxhash32(grid_coord_z + h2_xxhash32(grid_coord_y + h2_xxhash32(grid_coord_x)));
	unsigned int cell_hash = h1_pcg(grid_coord_z + h1_pcg(grid_coord_y + h1_pcg(grid_coord_x))) % total_number_of_cells;

	out_checksum = checksum;
	return cell_hash;
}
```

The hash function computes both a hash-key 'cell_hash' that will be used to index the has grid buffer and a checksum that can be used to detect collisions: it can happen that two very different
positions in world space end up yielding the same 'cell_hash', but chances are that the 'checksum' is going to be different, allowing us to detect the collision. If it happens that the checksum
is also the same, that's very unfortunate on top of that the checksum and we'll end up using the reservoirs of one grid cell to shade another cell in the grid. This is not the end of the world but
that's going to increase variance when that happens. 'checksum' uses a full 32 bit unsigned integer however so this situation should happen extremely infrequently. 'cell_hash' on the other hand is
computed modulo 'total_number_of_cells'. This is such that we do not get an index out of the hash function that does not fit in our hash grid buffer ('total_number_of_cells'
is the size of our hash grid buffer, allocated to a fixed size in advance)).

Resolving collisions is done with linear probing as presented by Binder et al. in their talk <d-cite key="binderPSF2019talk"></d-cite>., it's very simple although could use a bit more effectiveness. My testings suggest
that the hash table starts to struggle a bit with collisions at ~85% load factor (load factor is the proportion of cells occupied in the table). Considering that my tests were done with a maximum
of 32 steps for linear probing, that's not amazing when compared to the results of the survey on GPU hash tables from Awad et al. 2021 <d-cite key="awad2021bettergpuhashtables"></d-cite>. Their introduction report
a probe count of only 1.43 at load factor 99% for a BCHT (bucketed cuckoo hash table) scheme, but linear probing is already struggling at load factor 95% with a probe count of 32...

The consequence of a probing scheme that struggles is that we need to maintain a relatively low load factor to avoid suffering from unresolved collisions and lower performance because of longer probe
sequences. This means more wasted VRAM as the hash grid buffer must be larger than necessary to achieve a low enough load factor. My implementation resizes the buffer of the hash grid (and reinserts
the grid cells of the old hash table in the new hash table) automatically if the load factor exceeds 60% and I've found this enough of a "target load factor" in practice.

<div id="regir-hash-grid-collisions" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-hash-grid-collisions");
	
	if (dataHashGridCollisions['imageBoxes'])
		new ImageBox(content, dataHashGridCollisions['imageBoxes'], 1279, 692);
</script>

The above screenshots show that the number of collisions left unresolved increases as the load factor increases. Unresolved collisions manifest as black cells in the debug view. When doing NEE, this means that
we will not be able to access the hash grid data at all for that cell. That cell will thus be completely black as NEE will not have any light sample to work with. The solution that I've found to
that is just to fallback to a default light sampling strategy if that happens. This will have higher variance than ReGIR obviously (that's the goal after all, we want ReGIR to be better than just a
simple light sampling strategy) but at least it will not be conspicuously biased.

What's the impact of the hash grid resolution on quality? Well, let's have a look:

<div id="regir-hash-grid-size-quality" style="display: grid; justify-content: center; align-items: center;"></div>
<script>
	content = document.getElementById("regir-hash-grid-size-quality");
	
	if (dataHashGridSizeQuality['imageBoxes'])
		new ImageBox(content, dataHashGridSizeQuality['imageBoxes'], 1279, 692);
</script>

This hash grid saves us a ton of VRAM but also, very importantly, allows us to have geometric information about the cells of the grid: because we allocate grid in the buffer when a ray hits
the scene (we hash the position and insert in the right place if that grid cell hasn't been inserted in the hash grid before), we only ever store grid cells on the actual geometry of the scene
and we know what the geometry is like since we used geometric information to compute the hash in the first place. That geometric information attached to the grid cells is something that will come
in very handy for improving the quality of ReGIR samples.

Talk Normals in the hash function

## Improving the target function
	- Representative cell data --> get closer to the integrand with cosine terms and everything
	
## NEE++

## BSDF resampling
		- Better product sampling options out there
		- Doesn't work at secondary hits
		- Roughness in hash function
		
## Spatial reuse

## Temporal reuse

Not used because of correlations

## Fixing the bias
	- With cosine terms and everything in the grid fill target function, we now have an issue because some lights may be rejected -> need canonical samples
		- We need good MIS weights so that canonical sample don't add too much noise
	- Bias on perfect mirror surfaces: we need BSDF samples MIS at shading time
		- MIS: New multi-pairwise MIS for shading

## Multiple Importance Sampling
	- MIS also helps for weighting canonical and non canonical candidates
	- Multi pairwise MIS

## Reducing correlations
	- Correlation reduction
	- More reservoirs per cell
	- Better initial samples

## Improving the base samples
	- Cache cells
	- Variance issue when we have a ton of lights contributing to the same cell but our alias table can only contain so many lights so the rest has to be sample with a fallback sampling technique which adds a ton of noise
	
## VRAM optimizations for cache cells

## Visibility in the shading target function

# ReSTIRring ReGIR

# Limitations and potential improvements

## Correlations
	- More reservoirs per cell
	- Using reservoirs of past frames to improve the effective reservoir count at frame N
		- Doesn't work when accumulating because we now have temporal correlations in between frames
		
## Choosing the grid size
	- Improving cache plkaceemnt for...
	- Envmap sampling integration
	- Antithetic sampling inspired by...

## Determinism
	- Should be a way around it, just haven't got to it yet
