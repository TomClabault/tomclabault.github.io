---
layout: distill
title: ReSTIR DI in a Path Tracer
date: 2024-08-22 14:45:00+0200
description: Explaining and implementing ReSTIR DI for many light sampling
tags: path-tracing
thumbnail: assets/img/blogs/per-pixel-adaptive-sampling/thumbnail.jpg
categories: hiprt-path-tracer
related_posts: false
related_publications: true

bibliography: blogs/restir-di.bib

@inproceedings{RISTalbot,
author = {Talbot, Justin and Cline, David and Egbert, Parris},
year = {2005},
month = {01},
pages = {139-146},
title = {Importance Resampling for Global Illumination},
journal = {Eurographics Symposium on Rendering},
doi = {10.2312/EGWR/EGSR05/139-146}
}

Ask quuestion for env map sampling to shaeg
---

<link rel="stylesheet" href="/assets/css/distill-width-override.css">

<!-- Scripts for the ImageBox -->
<link rel="stylesheet" href="/assets/css/ImageBox/ImageBox.css">
<script src="/assets/js/ImageBox/ImageBox.js"></script>
<script src="/assets/blogs-assets/Per-Pixel-Adaptive-Sampling/ImageBox/data.js"></script>
<!-- Scripts for the ImageBox -->

# The problem

ReSTIR or Reservoir Spatiotemporal Importance Resampling is an algorithm for reusing (resampling) already generated samples in a path tracer. In 2020, Bitterli et. al 
published<d-cite key="bitterli2020spatiotemporal"></d-cite> their first ReSTIR paper aimed at reusing light samples for direct lighting integration. In its more general form<d-cite key="Lin2022"></d-cite>,
ReSTIR is more akin to a framework that it is to an algorithm. It offers lots of freedom in what you can do and allows reusing way more than just light samples (entire paths for example).
We'll only focus on ReSTIR DI for this blog post as it lays out the very important foundations for understanding the more recent ReSTIR papers for
rendering<d-cite key="wyman2021rearchitecting"></d-cite><d-cite key="Ouyang2021pathresampling"></d-cite><d-cite key="Lin2022"></d-cite><d-cite key="sawhney2022decorrelating"></d-cite><d-cite key="kettunen2023conditional"></d-cite><d-cite key="zhang2024area"></d-cite>
and how to implement them.

So in a more concrete context, what is ReSTIR DI trying to achieve? Efficient many-light sampling. What is "many-light sampling"? Assuming you're already familiar with Next Event Estimation
(or sometimes direct lighting estimation) (some resources here: <d-cite key="TuWienNEE"></d-cite><d-cite key="TuWienNEEVideo"></d-cite><d-cite key="PBRBookSamplingLightSources"></d-cite>), what happens
when you have a large number of lights in your scene? More than a thousand? Such a large number is absolutely possible given the fact that emissive meshes are common in path tracing. You then have to consider
every emissive triangle of the mesh and that's where the number of lights of a scene can blow up.

[Screenshot the white room love]

With such a large number of lights in the scene, how to decide which one to sample when evaluating Next Event Estimation? Usually, we sample lights at random, uniformly.
What's then the probability that we sample a light that is actually visible to our point? It's very low, even more so with a realistic 3D scene setup where each point
in the scene only effectively sees a fraction of the lights because of the geometry of the scene occluding the rest.

[screenshot of white room light with probabilities of light written for a point next to the couch left foreground]

Sampling the lights according to their emission power and area is a better solution but it is not foolproof, especially
in the face of occlusion, again. What if the most powerful light of your scene is in another room? Sampling according to emitted power will favor this light even though it
is occluded. Its direct lighting contribution will be 0 and lots of samples will be wasted. This all translates into poor light sampling performance and very high variance.

[1 SPP the white room lights here to show very high variance] "Path tracing is so cool wow! It makes shiny pictures and everything!" Yeah, well... With close to 20k emissive triangles in the scene, a given point has a low probability of choosing a light that really matters at that point. This translates into very high variance.

Prior to ReSTIR, the state of the art for light sampling used to be algorithms where BVHs are built on the lights to allow sampling the more important lighs with a much lower
computational footprints <d-cite key="MoreauLightBVH2019"></d-cite>. While these techniques provide significant improvement compared to uniform or power-based light sampling, much
better can be done today.

# Resampled Importance Sampling (RIS)

Resampled Importance Sampling<d-cite key="RISTalbot"></d-cite> is the heart of ReSTIR. At its core, it is a technique that takes multiple samples $(X_1, ..., X_M)$ produced according to a **source distribution $p$** and "resamples" them
into one sample $Y$ that follows a distribution $\overline{p}$ proportional to a **given (user-defined) target function $\hat{p}$**. Said otherwise, RIS takes a bunch of samples $(X_1, ..., X_M)$
and outputs one new sample $Y$ that is distributed approximately proportionally to whatever function $\hat{p}$ we want.

Let's quickly dive into a concrete example applied to our many-light sampling problem before this gets all too theoretical:

Ultimately, when estimating direct lighting using next-event estimation, we're trying to solve the following integral at each point $P$ in our scene:

\begin{equation}
L_o(P, \omega_o) = \int_{A}^{}f(P, \omega_o, \omega_i)L_e(x)V(P\leftrightarrow x)cos(\theta)dA_x
\label{directLightingIntegral}
\end{equation}

With:
- $L_o(P, \omega_o)$ the reflected radiance of point $P$ in direction $\omega_o$ (which can be the direction to the camera for example)
- $\int_{A}$ is the integral over the surface of our lights in the scene: to estimate the radiance that our point $P$ receives from the lights of the scene, we need a way of taking into account
all the lights of the scene. One way of doing that is to take all the lights and then consider all the points on the surface of these lights.
"Considering all the points of the surfaces of all the lights" is what we're doing here when integrating over $A$. $A$ can then be thought of as the union of
the area of all the lights and we're taking a point $dA_x$ on this union of areas.
- $f(P, \omega_o, \omega_i)$ is the BSDF used to evaluate the reflected radiance at point $P$ in direction $\omega_o$ of the light coming from direction $\omega_i$ (which is $x - P$).
- L_e(x) is the emission intensity of the point $x$
- $V(P\leftrightarrow x)$ is the visibility term that evaluates whether our point $P$ can see the point $x$ on the surface of a light or not (is it occluded by some geometry in the scene).
- $cos(\theta)$ is the attenuation term
- $x$ is a point on the surface of a light source

Now again, what a naive path tracer does to solve this integral is: at each point $P$ in the scene, choose a light $l$ at random (uniformly, power-based, ...), choose a point $x$ on the surface of this
light and evaluate equation **(\ref{directLightingIntegral})**.

With the lights chosen uniformly, the probability of chosing the light $l$ is $\frac{1}{\| Lights\|}$ and the probability of uniformly choosing the point $x$ on the surface of that light $l$ is $\frac{1}{Area(l)}$
with $\|Lights\|$ the number of lights in the scene. 

Our **source distribution $p$** is then defined as:

\begin{equation}
p(x) = \frac{1}{\|Lights\| * Area(l)}
\label{sourceDistributionP}
\end{equation}

That explains the first part of the sentence about RIS: "it is a technique that takes multiple samples $(X_1, ..., X_M)$ produced according to a **source distribution $p$**". We've got our source distribution $p$
and we can generate samples (points on light sources) with it. 

The next step is then to use RIS to "resample them into one sample that follows a distribution $\overline{p}$ proportional to a **given (user-defined) target function $\hat{p}$**."

So first of all, what's going to be our user-defined target function $\hat{p}$? Why not choose equation **(\ref{directLightingIntegral})** to be our target function so that RIS outputs
samples that are proportional exactly to the function that we're trying to integrate (which is ultimately the goal)? We can totally do that but it will not be efficient in practice (because of the visibility term that
requires tracing a ray, an expensive operation).  The target function $\hat{p}$ need to be cheap-ish to evaluate in practice or
RIS will turn out to be computationally inefficient.

So if the full equation **(\ref{directLightingIntegral})** is too expensive because of the visibility term, let's define our target function $\hat{p}$ without that visibility term:

\begin{equation}
\hat{p}(x) = f(P, \omega_o, \omega_i)L_e(x)cos(\theta)
\label{targetFunctionPHat}
\end{equation}

Now what do we do with our target function? We're going to use it to "resample" our samples $(X_1, ..., X_M)$ produced from $p$.

Here's the pseudocode of the RIS algorithm:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/restir-di/RISAlgo.png" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Resampled Importance Sampling (RIS) pseudo-code algorithm. Source: "A Gentle Introduction to ReSTIR", 2023<d-cite key="Wyman2023Gentle"></d-cite>
</div>

Algorithm starts at line 10. The idea is simple.

- (Line 10-11) Generate our $M$ candidates according to the distribution $p$ we talked about earlier. This corresponds to picking points on the surface of the lights of our scene.
- (Line 12) Compute the "resampling weight" $w_i$ of this candidate $X_i$. 
	- $m_i$ is a MIS weight, will be introduced in the next sections. For now, we'll use $m_i=\frac{1}{M}$
	- $\hat{p}(X_i)$ is the value of the target function for the generated sample $X_i$
	- $W_{X_i}$ is what we call the "unbiased contribution weight" of the sample/candidate. This term will be thoroughly detailed in the next sections.
	For now, we'll use $W_{X_i}=p(X_i)$, the probability of picking *that* point $X_i$ on *the* light in the scene the point belongs to.
- (Line 14) With these weights $w_i$, we are now going to choose a sample $X_i$ proportionally to its weight $w_i$: 
	- Let's say we have $M=3$ candidates. We compute their resampling weights $w_i$ and obtain $(w_1, w_2, w_3)=(1.2, 2.5, 0.5)$. The *randomIndex()* function can then be
	used to obtain the sample $X_i$ (it uses a non-precomputed CDF inversion technique for choosing that sample <d-cite key="PBRBookSampling1DFunction"></d-cite>). Concretely in this example, the probability of choosing $X_1$, $X_2$ or $X_3$ with the random index $s$ is then:
	
$$ p(s=0)=\frac{1.2}{1.2+2.5+0.5}=0.2857 $$

$$ p(s=1)=\frac{2.5}{1.2+2.5+0.5}=0.595 $$

$$ p(s=2)=\frac{2.5}{1.2+2.5+0.5}=0.119 $$

With the random index $s$ chosen, we can retrieve the final sample $Y$: the output of RIS. This sample $Y=X_s$ represents a combination of the $M$ samples that we input to RIS such that this sample $Y$ (and its weight $W_Y$ which we'll get to in a moment) follows a distribution $\overline{p}$ (which we call the "target PDF") proportional to the "target function" $\hat{p}$ that we chose. 

PUT A WARNING CODE BLOCK WARNING BLOKC THINGY HERE AS IN THE PER PXIEL ADAPTIVE SAMPLING NOTE

Note that "target PDF" and "target function" are two different concepts. The first ReSTIR paper<d-cite key="bitterli2020spatiotemporal"></d-cite> used the term "target PDF" in place of "target function" but we know since then<d-cite key="Lin2022"></d-cite> that this was a slight semantic mistake: the target function $\hat{p}$ is given by the user (that's you) and may not be normalized. The $Y$ sample produced, however, follows the normalized distribution $\overline{p}$ of $\hat{p}$ which is normalized (and thus adequate for use as a PDF) and that is the "target PDF".

Now, in Monte Carlo integration, we need the probability (PDF) of sampling a given sample. This also applies to next event estimation as you may know: when you sample a light in the scene, you divide by the probability of sampling that light when evaluating the radiance contribution of that light at the point you're shading.

RIS is no exception to that. RIS returns a light sample $Y$ and its weight $W_Y$ from the many $M$ light samples that we fed it. If we want to use that light sample $Y$ for evaluating direct lighting contribution, we're going to need its PDF. The hard truth is that RIS "mixes $M$ samples together" and as a result, we can't easily compute<d-cite key="bitterli2020spatiotemporal"></d-cite> the PDF of the resulting sample $Y$. What we can compute however, is an estimate of its PDF. That's what the $W_Y$ computed at line 17 of the algorithm is. This estimate converges to the inverse PDF $\frac{1}{\overline{p}(Y)}$ of $Y$ as the number of candidates $M$ resampled increases. If we then want to divide by the PDF of our sample $Y$, we can instead multiply by that estimate of the inverse PDF $W_Y$ and achieve the same result.

As a matter of fact, a sample output by RIS can be used for evaluation as simply as:

\begin{equation}
f(Y)W_Y
\end{equation}

that is: evaluate the function you want to integrate with the sample $Y$ and multiply by $W_Y$ (the estimate of its inverse PDF $\frac{1}{\overline{p}(Y)}$).

For our direct lighting evaluation case, we can use $Y$ as:

L_o(P, \omega_o) = f(P, \omega_o, \omega_i)V(P\leftrightarrow Y)cos(\theta)W_Y

with $omega_i$ the direction towards the light sample (point on the surface of a light) $Y$.

One key property of RIS is that the more samples $M$ we feed it, the closer to the target PDF $\overline{p}$ the resulting sample $Y$'s distribution will be. This is the reason why ReSTIR algorithms work at all as we'll see.

[ADD IMAGE THAT SHOWS HOW RIS GETS CLOSER AND LOSER TO THARGET DISTRIBUTION] https://scholarsarchive.byu.edu/

If RIS takes 5 samples as input and returns only 1 of the 5 as output, how is that an improvement? The key difference is that the sample $Y$ (which is 1 of the 5 that we fed RIS) now has a weight $W_Y$ associated with it. That weight makes all the difference as it accounts for all the samples that RIS has seen and makes it so that the sample $Y$ isn't exactly $X_s$ with PDF $p(X_s) $anymore, it is a whole new sample with an UCW $W_Y$ (inverse PDF estimate) that is distributed closer to $\overline{p}$ (defined by our given $\hat{p}$) than any of the 5 samples alone.

One detail that I omitted so far is the unit of the weights $w_i$. Looking at the RIS algorithm above, $w_i$ seems to be float numbers. Yet, they are computed with our target function $\hat{p}$ which includes the BSDF term and that BSDF term is a color, not a float. In practice, we take the luminance of our target function to compute the $w_i$ weights and so our $w_i$ computation looks more like this:

\begin{equation}

w_i = m_i(X_i)luminance(\hat{p})W_{X_i}

\end{equation}

Talk about target function luminance

Explain why visibility term is expensive

# Weighted Resampled Sampling

# ReSTIR DI

# ReSTIR DI - Spatial Reuse

# ReSTIR DI - Temporal Reuse

# ReSTIR DI - Details and quirks
Jacobian
1/Z visibility reuse + visibility bias correction mandatory

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/per-pixel-adaptive-sampling/cornell_pbr_reference.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Modified caustics cornell box, reference render.
</div>

Half of the rays of this scene don't even intersect any geometry and directly end up in the environment where the color of the environment map is computed. The variance of the radiance of these rays is very low since a given camera ray direction basically always results in the same radiance (almost) being returned.

However, the same cannot be said for the reflective caustic (the emissive light panel reflecting off the mirror small box) at the top right of the Cornell box. A camera ray that hits this region of the ceiling then has a fairly low chance of bouncing in direction of the small box to then bounce directly in the direction of the light. This makes the variance of these rays very high which really slows down the convergence of this part of the scene. As a result, we would like to shoot more rays at these pixels than at other parts of the scene to help with the convergence.

Adaptive sampling allows us to do just that. The idea is to estimate the error of each pixel of the image, compare this estimated error with a user-defined threshold $$ T $$ and only continue to sample the pixel if the pixel's error is still larger than the threshold.

A very simple error metric is that of the variance of the luminance $$ \sigma^2 $$ of the pixel. In practice, we want to estimate the variance of a pixel across the $$ N $$ samples $$x_k$$ it has received so far.

The variance of $$ N $$ samples is usually computed as:

$$
\sigma^2 = \frac{1}{N}\sum_{k=1}^N (x_k - \mu) ^2
$$

However, this approach would imply keeping the average of each pixel's samples (which is the framebuffer itself so that's fine) as well as the values of all samples seen so far (that's not fine). Every time we want to estimate the error of a single pixel, we would then have to loop over all the previous samples to compute their difference with the average and get our variance $$\sigma^2$$. Keeping track of all the samples is infeasible in terms of memory consumption (that would be 2GB of RAM/VRAM for a mere 256 samples' floating-point luminance at 1080p) and looping over all the samples seen so far is computationally way too demanding.

The practical solution is to evaluate the running-variance of the $$ N $$ pixel samples $$ x_k $$:

$$
\sigma^2 = \frac{1}{N - 1} \left(\sum_{k=1}^N x_k^2 - \left( \sum_{k=1}^N x_k \right)^2\right)
$$

> ##### Note
>
> Due to the nature of floating point numbers, this formula can have some precision issues. [This](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm) Wikipedia article presents good alternatives.
{: .block-tip }

With the variance, we can compute a 95% confidence interval $$ I $$:

$$
I = 1.96 \frac{\sigma}{\sqrt{N}}
$$

This 95% confidence interval gives us a range around our samples mean $$ \mu $$ and we can be 95% sure that, for the current number of samples $$ N $$ and and their variance $$ \sigma $$ that we used to compute this interval, the converged mean (true mean) of an infinite amount of samples is in that interval.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/per-pixel-adaptive-sampling/confidenceInterval.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Visualization of the confidence interval $I$ (green arrows) around $\mu$.
</div>

Judging by how $$ I $$ is computed, it is easy to see that as the number of samples $$ N $$ increases or the variance $$ \sigma^2 $$ decreases (and thus $$ \sigma $$ decreases too), $$ I $$ decreases. 

That should make sense since as we increase the number of samples, our mean $$ \mu $$ should get closer and closer to the "true" mean value of the pixel (which is the value of the fully converged pixel when an infinite amount of samples are averaged together). 

If $$ I $$ gets smaller, this means for our $$ \mu $$ that it also gets closer to the "true" mean and that is the sign that our pixel has converged a little more.

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/per-pixel-adaptive-sampling/confidenceInterval2.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	As the number of samples increases (or as the computed variance decreases), $I$ gets smaller, meaning that the true mean is closer to our current mean which in turn means that our pixel has converged a little more.
</div>

Knowing that we can interpret $$ I $$ as a measure of the convergence of our pixel, the question now becomes: 

**When do we assume that our pixel has sufficiently converged and stop sampling?**

We use that user-given threshold $$ T $$ we talked about earlier! Specifically, we can assume that if $$ I \leq T\mu $$, rhen that pixel has converged enough for that threshold $$ T $$. As a practical example, consider $$ T=0 $$. We then have:

$$
\displaylines{I \leq T\mu \\ I \leq 0}
$$

If $$ I=0 $$, then the interval completely collapses on $$ \mu $$. Said otherwise, $$ \mu $$ **is** the true mean and our pixel has completely converged. Thus, for $$ T=0 $$, we will only stop sampling the pixel when it has fully converged.

In practice, having $$ I=0 $$ is infeasible. After some experimentations a $$ T $$ threshold of $$ 0.1 $$ seems to target a visually very reasonable amount of noise. Any $$ T $$ lower than that represents quite the overhead in terms of rendering times but can still provide some improvements on the perceived level of noise:

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/per-pixel-adaptive-sampling/cornellThreshold.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Comparison of the noise level obtained after all pixels have converged and stopped sampling with a varying $T$ threshold.
</div>

Now if you look at the render with $$T=0.1$$, you'll notice that the caustic on the ceiling is awkwardly noisier than the rest of the image. There are some "holes" in the caustic (easy to see when you compare it to the $$T=0.05$$ render).

This is an issue of the per-pixel approach used here: because that caustic has so much variance, it is actually possible that we sample a pixel on the ceiling 50 times (arbitrary number)
without ever finding a path to the light. The sampled pixel will then remain gray-ish (diffuse color of the ceiling) instead of being bright because of the caustic. Our evaluation of the
error of this pixel will then assume that it has converged since it has gone through 50 samples without that much of a change in radiance, meaning that it has a low variance, meaning that we can stop sampling it. 

But we shouldn't! If we had sampled it maybe 50 more times, we would have probably found a path that leads to the light, spiking the variance of the pixel which in turn would be sampled until the variance has attenuated enough so that our confidence interval $$I$$ is small again and gets below our threshold.

One solution is simply to increase the minimum number of samples that must be traced through a pixel before evaluating its error. This way, the pixels of the image all get a chance to show their true variance and can't escape the adaptive sampling strategy! 

<div class="col-sm mt-3 mt-md-0">
		{% include figure.liquid path="assets/img/blogs/per-pixel-adaptive-sampling/minimumSampleNumber.jpg" class="img-fluid rounded z-depth-1" zoomable=true %}
</div>
<div class="caption">
	Impact of the minimum amount of samples to trace before starting evaluating adaptive sampling for the same $T$ threshold.
</div>

This is however a poor solution since this forces all pixels of the image to be sampled at least 100 times, even the ones that would only need 50 samples. This is a waste of computational resources.

A better way of estimating the error of the scene will be presented in a future blog post on "Hierarchical Adaptive Sampling".

Nonetheless, this naive way of estimating the error of a pixel can provide very appreciable speedups in rendering time. 

<div id="ImageBoxContent"></div>
<script>
	content = document.getElementById("ImageBoxContent");
	
	if (data['imageBoxes'])
		new ImageBox(content, data['imageBoxes'], 1280, 720);


	// if (data['stats'])
		//new ChartBox(content, data['stats']);
		//new TableBox(content, data['stats']);
</script>

|           $T = 0.1$           	|           **90%** 	|           **95%** 	|               **99%** 	|
|:-----------------------------:	|------------------:	|------------------:	|----------------------:	|
|           **Bistro**          	| 3626/1601 (2.26x) 	| 5104/1782 (2.86x) 	| 12068/2057 (**5.8x**) 	|
|         **McLaren P1**        	|   178/74 (2.4x)   	|  492/198 (2.48x)  	|  3227/480 (**6.7x**)  	|
|  **The White Room 4 bounces** 	|    98/54 (1.8x)   	|   130/58 (2.24x)  	|   270/78 (**3.46x**)  	|
| **The White Room 16 bounces** 	|  1020/347 (2.9x)  	|  1668/656 (2.54x) 	|  2629/459 (**5.7x**)  	|
|                               	|                   	|                   	|                       	|
|            $T=0.3$            	|      **90%**      	|      **95%**      	|        **99%**        	|
|           **Bistro**          	|  384/152 (2.53x)  	|  551/170 (3.24x)  	|  1124/196 (**5.73x**) 	|
|         **McLaren P1**        	|    17/10 (1.7x)   	|   43/19 (2.26x)   	|   244/50 (**4.88x**)  	|
|  **The White Room 4 bounces** 	|    10/6 (1.67x)   	|    15/7 (2.14x)   	|    31/8 (**3.88x**)   	|
| **The White Room 16 bounces** 	|   69/26 (2.65x)   	|   111/31 (3.58x)  	|   268/39 (**6.87x**)  	|


A few notes:

- Times are in second
- All renders with adaptive sampling were rendered with 96 minimum samples.
- Do not compare times between categories (categories are 90%/95%/99% and $$T=0.1$$ or $$T=0.3$$). Some images were rendered
with the GPU limiter feature of my renderer (to avoid burning my GPU for hours) and so times are not always comparable between
categories. Two images of the same category (and of the same scene, i.e. do not compare times of the Bistro with times of the
Mc Laren P1) were rendered with the exact same settings so times are comparable there.

- You may have noticed that the images with that used the adaptive sampling are noisier than the images rendered without
(most noticeable with $$T=0.3$$). This is because without the adaptive sampling, the renderer had no choice but to render
<strong>every</strong> pixels of the image until 90/95/99% have reached the threshold ($$T=0.1$$ or $$T=0.3$$). This is a
massive waste of time, especially if you intend to use a denoiser as the final step of the rendering process. As a matter
of fact, Open Image Denoise (which is the denoiser in my renderer used at the time of writing this post) with normals and
albedo AOVs behaves very well on an image rendered with a $$T=0.3$$ threshold and adaptive sampling on. There's almost no
reason not to use adaptive sampling when denoising the render (except maybe for the "holes" issue explained below).

- The holes are still there! Even at $$T=0.1$$, on the Mc Laren, below the right mirror :(
Unfortunately yes. The only proper solution that I can imagine is to either increase the minimum number of samples before
kicking off the adaptive sampling or use a hierarchical solution<d-cite key="jefferyHierarchicalAdaptiveSampling"></d-cite>,
which I haven't explored in practice yet.
